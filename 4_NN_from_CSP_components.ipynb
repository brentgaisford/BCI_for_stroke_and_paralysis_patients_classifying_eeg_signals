{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ae01077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T20:42:17.584205Z",
     "start_time": "2022-07-19T20:42:17.573235Z"
    }
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout)\n",
    "\n",
    "import itertools\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ffdd8",
   "metadata": {},
   "source": [
    "**Run our data ingester to bring our initial data in**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe923f8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T20:40:14.159113Z",
     "start_time": "2022-07-19T20:39:38.512081Z"
    }
   },
   "outputs": [],
   "source": [
    "%run data_ingester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e023f8",
   "metadata": {},
   "source": [
    "### In this notebook, we'll build L1 models from neural networks using input from higher-dimension CSP components to capture a bit more signal for our final NN\n",
    "\n",
    "Similarly to the LDA models, we'll shoot for accuracy of about 75%. However, because shallow NN are not as good at interpreting the CSP components as LDA models, we should be able to use a higher number of CSP components, and capture a different signal to pass into our ensemble.\n",
    "\n",
    "We'll use the MNE parameters from our LDA/CSP models as the basis for testing NN on CSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40da7c12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T21:10:23.637017Z",
     "start_time": "2022-07-19T21:10:23.604108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slice(None, 1, None)\n",
       "1    slice(None, 1, None)\n",
       "2    slice(None, 1, None)\n",
       "3       slice(1, 2, None)\n",
       "4    slice(None, 1, None)\n",
       "5    slice(None, 1, None)\n",
       "6    slice(None, 1, None)\n",
       "7    slice(None, 1, None)\n",
       "8       slice(1, 2, None)\n",
       "Name: projectors_to_apply, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import final CSP model params\n",
    "base_params = pd.read_csv('data/csp_models_not_overfit.csv')\n",
    "\n",
    "#Reset several columns away from strings and as literal_evals so can be read\n",
    "base_params['trial_combo'] = (base_params['trial_combo'].\n",
    "                                apply(lambda x: literal_eval(x)))\n",
    "\n",
    "#Use list comprehension b/c NaN can appear in list\n",
    "base_params['flat'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in base_params['flat']]\n",
    "base_params['reject'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in base_params['reject']]\n",
    "\n",
    "base_params['projectors_to_apply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f0b084c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T21:10:26.226248Z",
     "start_time": "2022-07-19T21:10:26.180375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 20)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_params['projectors_to_apply'] = [slice(None, 1, None),\n",
    "                                      slice(None, 1, None),\n",
    "                                      slice(None, 1, None),\n",
    "                                      slice(1, 2, None),\n",
    "                                      slice(None, 1, None),\n",
    "                                      slice(None, 1, None),\n",
    "                                      slice(None, 1, None),\n",
    "                                      slice(None, 1, None),\n",
    "                                      slice(1, 2, None)]\n",
    "\n",
    "base_params['test_acc'] = None\n",
    "base_params['train_acc'] = None\n",
    "\n",
    "#Columns to drop that we want to overwrite with new test params\n",
    "base_params.drop(['n_components', 'cov_est', 'log'], \n",
    "                 inplace=True,\n",
    "                 axis=1)\n",
    "\n",
    "#Replace NaNs with Nones so MNE reads them properly\n",
    "base_params.replace(np.nan, None, inplace=True)\n",
    "\n",
    "#Create our test frame of parameters we want to iterate over\n",
    "n_components_options = [8, 12, 16]\n",
    "cov_est_options = ['concat', 'epoch']\n",
    "log_options = [True, False]\n",
    "\n",
    "columns = ['n_components',\n",
    "           'cov_est',\n",
    "           'log']\n",
    "\n",
    "#Create dataframe of our variable params\n",
    "variable_params = pd.DataFrame(itertools.\n",
    "                               product(n_components_options,\n",
    "                                       cov_est_options,\n",
    "                                       log_options), \n",
    "                           columns=columns)\n",
    "\n",
    "#Get number of permutations of variable parameters\n",
    "permutations = len(list(itertools.\n",
    "                        product(n_components_options,\n",
    "                                cov_est_options,\n",
    "                                log_options)))\n",
    "\n",
    "#Duplicate our temp frame to match the number of variable\n",
    "#permutations to run each permutation for each subject\n",
    "temp_columns = base_params.columns\n",
    "base_params = pd.DataFrame(np.repeat(base_params.values, \n",
    "                                     permutations, \n",
    "                                     axis=0))\n",
    "base_params.columns = temp_columns\n",
    "\n",
    "#Concat variable params with itself 9 times times to get\n",
    "#right shape to combine with all params for all subjects\n",
    "nn_csp_df = pd.concat((variable_params, \n",
    "                              variable_params,\n",
    "                              variable_params,\n",
    "                              variable_params,\n",
    "                              variable_params,\n",
    "                              variable_params,\n",
    "                              variable_params,\n",
    "                              variable_params,\n",
    "                              variable_params),\n",
    "                              ignore_index=True)\n",
    "\n",
    "#Join our two grids to assemble full ensemble test grid\n",
    "nn_csp_df = base_params.join(nn_csp_df)\n",
    "\n",
    "nn_csp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44edfbc8",
   "metadata": {},
   "source": [
    "### Let's construct the function we'll use to iterate across our test dataframe and find the right parameters for our NN / CSP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d266e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T21:07:17.952647Z",
     "start_time": "2022-07-19T21:07:17.892820Z"
    }
   },
   "outputs": [],
   "source": [
    "def NN_csp_grid_search(test_df, savefile):\n",
    "    \"\"\"A function to iterate across a test\n",
    "    dataframe and fill in the resulting\n",
    "    train and test scores using those parameters.\n",
    "    \n",
    "    The test dataframe must be constructed with a \n",
    "    subject column, i.e., one subject per row.\n",
    "    \n",
    "    Savefile is the filename to use in saving\n",
    "    test dataframe to csv in data directory\n",
    "    each iteration.\"\"\"\n",
    "    for row in range(test_df.shape[0]):\n",
    "        #Load each sessions data into an MNE raw object\n",
    "        raw_dict = {}\n",
    "        for key, value in data_dict.items():\n",
    "            if (test_df.subject[row] in key) and ('sesh_1' in key):\n",
    "                raw_dict[key] = mne.io.RawArray(value.T, info, verbose=0)\n",
    "\n",
    "\n",
    "        #Filter data with bandpass. Note raw.filter applies in place\n",
    "        for key, value in raw_dict.items():\n",
    "            value.filter(l_freq=test_df.l_freq_filter[row], \n",
    "                         h_freq=test_df.h_freq_filter[row], \n",
    "                         method='fir', phase='zero', verbose=0)\n",
    "\n",
    "        #Create epoch object with our raw objects and events arrays\n",
    "        channels_to_keep = [ch for ch in ch_names if \n",
    "                            ch not in test_df.channels_to_drop[row]]\n",
    "        epoch_dict = {}\n",
    "        for key, value in raw_dict.items():\n",
    "            epoch_dict[key] = mne.Epochs(value, events=event_dict[key], \n",
    "                                        event_id=events_explained, \n",
    "                                        tmin=-3, tmax=test_df.tmax[row], \n",
    "                                        baseline=test_df.baseline_correction[row],\n",
    "                                        preload=True,\n",
    "                                        picks=channels_to_keep, verbose=0,\n",
    "                                        detrend=test_df.detrend[row],\n",
    "                                        reject=test_df.reject[row],\n",
    "                                        flat=test_df.flat[row],\n",
    "                                        reject_tmin=test_df.tmin[row],\n",
    "                                        reject_tmax=test_df.tmax[row])\n",
    "\n",
    "        #Skip creating projectors step to save compute time if not being\n",
    "        #applied in this iteration\n",
    "        if test_df.projectors_to_apply[row]:\n",
    "            #Create dictionary of signal space projection vectors for each epoch\n",
    "            proj_dict = {}\n",
    "            for key, value in epoch_dict.items():\n",
    "                proj_dict[key] = mne.compute_proj_epochs(value, \n",
    "                                                         n_eeg=2, \n",
    "                                                         verbose=0)\n",
    "            #apply projectors\n",
    "            for key, value in epoch_dict.items():\n",
    "                value.add_proj(proj_dict[key][test_df.projectors_to_apply[row]], \n",
    "                               verbose=0)\n",
    "                value.apply_proj(verbose=0)\n",
    "\n",
    "        #Skip creating ICA components step to save compute time if not\n",
    "        #being applied in this iteration\n",
    "        if test_df.ica_to_exclude[row]:\n",
    "            #create and fit ICA object to epochs\n",
    "            for key, value in epoch_dict.items():\n",
    "                ica = mne.preprocessing.ICA(n_components=5, method='picard', \n",
    "                                            max_iter='auto', verbose=0)\n",
    "                ica.fit(value, verbose=0)\n",
    "                #Apply the ICA\n",
    "                ica.apply(value, exclude=test_df.ica_to_exclude[row],\n",
    "                         verbose=0)\n",
    "\n",
    "        #Resample the data at a new frequency, happens inplace\n",
    "        for key, value in epoch_dict.items():\n",
    "            value.resample(sfreq=test_df.selected_frequency[row])\n",
    "\n",
    "        #Extract and standard scale data from all non-dropped epochs\n",
    "        #Creates intermediate data dictionary\n",
    "        int_data_dict = {}\n",
    "        #Use robust sklearn scaler\n",
    "        if test_df.scaler[row] == 'robust':\n",
    "            mne_scaler = mne.decoding.Scaler(scalings='median')\n",
    "            for key, value in epoch_dict.items():\n",
    "                #with scalings=median implements sklearn robust scaler\n",
    "                int_data_dict[key] = (mne_scaler.\n",
    "                                      fit_transform(value.\n",
    "                                                    get_data(tmin=test_df.tmin[row], \n",
    "                                                             tmax=test_df.tmax[row])))\n",
    "        #No scaling option\n",
    "        if test_df.scaler[row] is None:\n",
    "            for key, value in epoch_dict.items():\n",
    "                int_data_dict[key] = value.get_data(tmin=test_df.tmin[row], \n",
    "                                                      tmax=test_df.tmax[row])\n",
    "\n",
    "        #Create updated dictionary of y values to reflect dropped epochs\n",
    "        int_y_dict = {}\n",
    "        for key, value in y_dict.items():\n",
    "            if (test_df.subject[row] in key) and ('sesh_1' in key):\n",
    "                temp_y_list = []\n",
    "                for i, epoch in enumerate(epoch_dict[key].drop_log):\n",
    "            #MNE drop log shows empty parens for epochs that were not dropped - \n",
    "            #these are the trials we are keeping in each iteration\n",
    "                    if epoch == ():\n",
    "                        temp_y_list.append(value[i])\n",
    "                int_y_dict[key] = temp_y_list\n",
    "\n",
    "        #Assemble final y dict with only trials in our current combo\n",
    "        #In each combo, coding 1st trial type to 0, 2nd trial type to 1\n",
    "        final_y_dict = {}\n",
    "        for key, value in int_y_dict.items():\n",
    "            temp_y_list = []\n",
    "            for y in value:\n",
    "                if y == test_df.trial_combo[row][0]:\n",
    "                    temp_y_list.append(0)\n",
    "                if y == test_df.trial_combo[row][1]:\n",
    "                    temp_y_list.append(1)\n",
    "            final_y_dict[key] = np.array(temp_y_list)\n",
    "\n",
    "        #Assemble data dict with only trials in our current combo\n",
    "        final_data_dict = {}\n",
    "        for key, value in int_data_dict.items():\n",
    "            index_list = []\n",
    "            for i, y in enumerate(int_y_dict[key]):\n",
    "                if (y == test_df.trial_combo[row][0] or \n",
    "                    y == test_df.trial_combo[row][1]):\n",
    "                    index_list.append(i)\n",
    "            final_data_dict[key] = value[index_list]\n",
    "\n",
    "        #Create csp_dict of csp objects\n",
    "        csp_dict = {}\n",
    "        for key, value in epoch_dict.items():\n",
    "            csp_dict[key] = mne.decoding.CSP(n_components=int(test_df.n_components[row]), \n",
    "                                                 cov_est=test_df.cov_est[row], \n",
    "                                                 log=bool(test_df.log[row]));\n",
    "\n",
    "        #Suppress output from this noisy function with no verbose option\n",
    "        with io.capture_output() as captured:\n",
    "        #Fit csp objects to training data from session 1        \n",
    "            for key, value in csp_dict.items():\n",
    "                #Try except to deal with iterations where fails to converge\n",
    "                try:\n",
    "                    value.fit(X=final_data_dict[key], \n",
    "                          y=final_y_dict[key]);\n",
    "                except:\n",
    "                    csp_dict[key] = 'CSP failed to converge'\n",
    "\n",
    "        #Use csp objects to transform and save resulting data\n",
    "        csp_data_dict = {}\n",
    "        for key, value in csp_dict.items():\n",
    "            #If except to deal with iterations where CSP fails to converge\n",
    "            if value == 'CSP failed to converge':\n",
    "                csp_data_dict[key] = 'CSP failed to converge'\n",
    "            else:\n",
    "                csp_data_dict[key] = value.transform(final_data_dict[key])\n",
    "\n",
    "        #Model against our data for each subject and save the resulting score\n",
    "        for key, value in csp_data_dict.items():\n",
    "\n",
    "            #Pass through CSP failure to ouput\n",
    "            if csp_dict[key] == 'CSP failed to converge':\n",
    "                test_df.at[row, 'test_acc'] = 'CSP failed to converge'\n",
    "                test_df.at[row, 'train_acc'] = 'CSP failed to converge'\n",
    "            #Else train test split our data\n",
    "            else:\n",
    "                (X_train, X_test, \n",
    "                 y_train, y_test) = train_test_split(value, \n",
    "                                                 final_y_dict[key], \n",
    "                                                 stratify=final_y_dict[key],\n",
    "                                                 random_state=23)\n",
    "            \n",
    "                #Build model\n",
    "                model = Sequential()\n",
    "                #inputs qre equal to n_components created via CSP\n",
    "                model.add(Dense(test_df.n_components[row], \n",
    "                                input_dim=test_df.n_components[row], \n",
    "                                activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                #Add hidden layer with half as many nodes as input\n",
    "                model.add(Dense(int(test_df.n_components[row]/2), \n",
    "                                activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                #Hidden layer with 1/4 as many nodes as input\n",
    "                model.add(Dense(int(test_df.n_components[row]/4), \n",
    "                                activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                #output layer\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "                #Suppress output\n",
    "                with io.capture_output() as captured:\n",
    "                    #Compile model\n",
    "                    model.compile(loss='binary_crossentropy', \n",
    "                                  optimizer='adam', \n",
    "                                  metrics=['acc'])\n",
    "\n",
    "                    #Fit model\n",
    "                    key2 = key.replace('1', '2')\n",
    "                    history = model.fit(X_train, y_train, \n",
    "                                        validation_data=(X_test, \n",
    "                                                         y_test), \n",
    "                                        epochs=3, verbose=0)\n",
    "\n",
    "                #Save validation accuracy into dataframe\n",
    "                test_df.at[row, 'test_acc'] = max(history.history['val_acc'])\n",
    "                test_df.at[row, 'train_acc'] = max(history.history['acc'])\n",
    "\n",
    "        test_df.to_csv(f'data/{savefile}.csv', index=False)\n",
    "        if row % 50 == 0:\n",
    "            print(f'Grid search complete through row {row} of {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fde1e2",
   "metadata": {},
   "source": [
    "### Let's run through our test frame and find models that match our 75% accuracy goal, and with train and test accuracies that are quite close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caf99db2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T21:21:07.548229Z",
     "start_time": "2022-07-19T21:10:31.326129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete through row 0 of 108\n",
      "Grid search complete through row 50 of 108\n",
      "Grid search complete through row 100 of 108\n"
     ]
    }
   ],
   "source": [
    "NN_csp_grid_search(test_df=nn_csp_df, savefile='NN_from_CSP_gridsearch_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73d5ee8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T22:57:49.157342Z",
     "start_time": "2022-07-19T22:57:49.088543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial_combo</th>\n",
       "      <th>flat</th>\n",
       "      <th>reject</th>\n",
       "      <th>baseline_correction</th>\n",
       "      <th>detrend</th>\n",
       "      <th>ica_to_exclude</th>\n",
       "      <th>scaler</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>l_freq_filter</th>\n",
       "      <th>h_freq_filter</th>\n",
       "      <th>channels_to_drop</th>\n",
       "      <th>projectors_to_apply</th>\n",
       "      <th>selected_frequency</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>n_components</th>\n",
       "      <th>cov_est</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>12</td>\n",
       "      <td>epoch</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>8</td>\n",
       "      <td>concat</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>16</td>\n",
       "      <td>concat</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8</td>\n",
       "      <td>epoch</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>12</td>\n",
       "      <td>concat</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>12</td>\n",
       "      <td>concat</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>16</td>\n",
       "      <td>concat</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>16</td>\n",
       "      <td>epoch</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>12</td>\n",
       "      <td>epoch</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>8</td>\n",
       "      <td>concat</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject trial_combo           flat        reject baseline_correction  \\\n",
       "103   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "97    sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "104   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "99    sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "101   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "100   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "105   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "106   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "102   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "96    sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "\n",
       "    detrend ica_to_exclude  scaler  test_acc l_freq_filter h_freq_filter  \\\n",
       "103    None           None  robust  0.842105           1.0          40.0   \n",
       "97     None           None  robust  0.578947           1.0          40.0   \n",
       "104    None           None  robust  0.473684           1.0          40.0   \n",
       "99     None           None  robust  0.526316           1.0          40.0   \n",
       "101    None           None  robust  0.526316           1.0          40.0   \n",
       "100    None           None  robust  0.789474           1.0          40.0   \n",
       "105    None           None  robust  0.631579           1.0          40.0   \n",
       "106    None           None  robust  0.473684           1.0          40.0   \n",
       "102    None           None  robust  0.526316           1.0          40.0   \n",
       "96     None           None  robust  0.473684           1.0          40.0   \n",
       "\n",
       "                                      channels_to_drop projectors_to_apply  \\\n",
       "103  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "97   ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "104  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "99   ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "101  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "100  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "105  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "106  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "102  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "96   ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "\n",
       "    selected_frequency tmin tmax train_acc  n_components cov_est    log  \n",
       "103                256    1  5.5  0.709091            12   epoch  False  \n",
       "97                 256    1  5.5  0.636364             8  concat  False  \n",
       "104                256    1  5.5  0.636364            16  concat   True  \n",
       "99                 256    1  5.5       0.6             8   epoch  False  \n",
       "101                256    1  5.5       0.6            12  concat  False  \n",
       "100                256    1  5.5  0.581818            12  concat   True  \n",
       "105                256    1  5.5  0.581818            16  concat  False  \n",
       "106                256    1  5.5  0.553571            16   epoch   True  \n",
       "102                256    1  5.5  0.545455            12   epoch   True  \n",
       "96                 256    1  5.5  0.509091             8  concat   True  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_csp_df[nn_csp_df['subject'] == 'sub_L'].sort_values(\n",
    "    'train_acc', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf11cdd",
   "metadata": {},
   "source": [
    "**Based on the dataframe above, let's assemble our selections of final NN/CSP models**\n",
    "\n",
    "Looked through the list of output manually to find models that were very similar on train and test accuracy and 75-80% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ad7a250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T22:58:18.268514Z",
     "start_time": "2022-07-19T22:58:18.254301Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_index_list = [3, 16, 35, 41, 50, 65, 81, 88, 103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b47bd856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-19T22:59:04.980641Z",
     "start_time": "2022-07-19T22:59:04.956705Z"
    }
   },
   "outputs": [],
   "source": [
    "nn_csp_df.iloc[selected_index_list].to_csv('data/NN_CSP_models_not_overfit.csv',\n",
    "                                          index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mne+tensorflow]",
   "language": "python",
   "name": "conda-env-mne_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
