{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb778ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T19:41:08.931406Z",
     "start_time": "2022-07-17T19:40:53.023216Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 15:27:11.209845: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-19 15:27:11.209884: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Flatten, Conv1D, \n",
    "                                     MaxPooling1D, GlobalAveragePooling1D)\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c548c",
   "metadata": {},
   "source": [
    "### Purpose of this notebook\n",
    "\n",
    "In this notebook we will be testing and creating the highest-performing individualized CNN models we can based on session 1 data, which will then be used as one of the L1 models in my final ensemble model to make our predictions on session 2 data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0650e4",
   "metadata": {},
   "source": [
    "### Import our data with the data ingester\n",
    "\n",
    "I built a script to do this ingestion - please see the script file in the project repo for code.\n",
    "\n",
    "After running, this script will load several dictionaries into memory, as well as other needed objects:\n",
    "- data_dict - containing all the sample eeg data\n",
    "- event_dict - which indicates the sample number at which each stimulus was applied\n",
    "- y_dict - which has the type of experiment conducted in each trial\n",
    "- info - file used to create MNE raw objects including channel names, type, and sampling frequency\n",
    "- events_explained - dictionary which provides the names for each of the five trial types\n",
    "- ch_names - list of all channel names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdbf51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_ingester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be146ab6",
   "metadata": {},
   "source": [
    "### Create initial gridsearch to look for best signal processing parameters to use with our CNN models\n",
    "\n",
    "To begin, we'll start by importing our results from the CSP notebook. We need to use the same MNE parameters for all of our L1 models, so we'll plan to use all those settings with our CNN models.\n",
    "\n",
    "In order to avoid models that are too highly fit, I'll be looking to create two CNN models per subject. Both will be shallow models, and I will aim for one CNN model to identify larger features in the EEG data, and one to capture smaller features.\n",
    "\n",
    "Let's do a set of testing with that aim, and potentially another set thereafter if needed. In general, our goal is to create models with 75-80% accuracy on the test portion of our session 1 data, in the hopes those will be more generalizable models to bring into our ensemble.\n",
    "\n",
    "After that, we'll examine how many epochs we can run those models for before overfitting begins to occur, and that number of epochs to run will become the final parameter of our individualized CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502300e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-17T23:16:55.902077Z",
     "start_time": "2022-07-17T23:16:55.831198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slice(None, 1, None)\n",
       "1    slice(None, 1, None)\n",
       "2    slice(None, 1, None)\n",
       "3       slice(1, 2, None)\n",
       "4    slice(None, 1, None)\n",
       "5    slice(None, 1, None)\n",
       "6    slice(None, 1, None)\n",
       "7    slice(None, 1, None)\n",
       "8       slice(1, 2, None)\n",
       "Name: projectors_to_apply, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import final CSP model params to use as starting point\n",
    "locked_params = pd.read_csv('data/csp_models_not_overfit.csv')\n",
    "\n",
    "#Drop CSP variables not needed here, plus output column\n",
    "locked_params.drop(['n_components', 'cov_est', 'log', 'test_acc'],\n",
    "                   axis=1,\n",
    "                  inplace=True)\n",
    "\n",
    "#Reset trial combo to tuple, flat and reject to dicts\n",
    "locked_params['trial_combo'] = (locked_params['trial_combo'].\n",
    "                                apply(lambda x: literal_eval(x)))\n",
    "#Use list comprehension b/c NaN can appear in list\n",
    "locked_params['flat'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in locked_params['flat']]\n",
    "#Use list comprehension b/c NaN can appear in list\n",
    "locked_params['reject'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in locked_params['reject']]\n",
    "\n",
    "locked_params['projectors_to_apply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23337ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually overwrite projectors col, literal_eval doesn't work on slices\n",
    "locked_params['projectors_to_apply'] = [slice(None, 1, None),\n",
    "                                        slice(None, 1, None),\n",
    "                                        slice(None, 1, None),\n",
    "                                        slice(1, 2, None),\n",
    "                                        slice(None, 1, None),\n",
    "                                        slice(None, 1, None),\n",
    "                                        slice(None, 1, None),\n",
    "                                        slice(None, 1, None),\n",
    "                                        slice(1, 2, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efc09da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 27)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shallow CNN with fewer, larger filters\n",
    "sample_weight_options = [2, 3]\n",
    "#First Conv1D layer settings\n",
    "input_filter_count_options = [20, 30]\n",
    "input_kernel_size_options = [10, 15]\n",
    "input_strides_options = [6, 8]\n",
    "#Hidden Conv1D layer settings\n",
    "hidden_filters_options=[20, 30]\n",
    "hidden_kernel_size_options=[7] \n",
    "hidden_strides_options=[7]\n",
    "#Pooling layers settings\n",
    "pool_size_options=[4]\n",
    "#Penultimate dense layer settings\n",
    "nodes_options = [20, 30]\n",
    "model_type_options = ['Large_filters']\n",
    "\n",
    "\n",
    "columns = ['sample_weight',\n",
    "           'input_filter_count',\n",
    "           'input_kernel_size',\n",
    "           'input_strides',\n",
    "           'hidden_filters',\n",
    "           'hidden_kernel_size',\n",
    "           'hidden_strides',\n",
    "           'pool_size',\n",
    "           'nodes',\n",
    "           'model_type']\n",
    "\n",
    "#Get number of permutations of variable parameters\n",
    "permutations_1 = len(list(itertools.\n",
    "                        product(sample_weight_options, \n",
    "                                input_filter_count_options,\n",
    "                                input_kernel_size_options,\n",
    "                                input_strides_options,\n",
    "                                hidden_filters_options,\n",
    "                                hidden_kernel_size_options,\n",
    "                                hidden_strides_options,\n",
    "                                pool_size_options,\n",
    "                                nodes_options,\n",
    "                                model_type_options)))\n",
    "\n",
    "#Create dataframe of our first set of variable params\n",
    "variable_params = pd.DataFrame(itertools.\n",
    "                               product(sample_weight_options, \n",
    "                                input_filter_count_options,\n",
    "                                input_kernel_size_options,\n",
    "                                input_strides_options,\n",
    "                                hidden_filters_options,\n",
    "                                hidden_kernel_size_options,\n",
    "                                hidden_strides_options,\n",
    "                                pool_size_options,\n",
    "                                nodes_options, \n",
    "                                model_type_options), \n",
    "                           columns=columns)\n",
    "\n",
    "#Shallow CNN with more, smaller filters\n",
    "sample_weight_options_2 = [2, 3]\n",
    "#First Conv1D layer settings\n",
    "input_filter_count_options_2 = [45, 55]\n",
    "input_kernel_size_options_2 = [3, 6]\n",
    "input_strides_options_2 = [1, 2]\n",
    "#Hidden Conv1D layer settings\n",
    "hidden_filters_options_2 = [45, 55]\n",
    "hidden_kernel_size_options_2 = [3] \n",
    "hidden_strides_options_2 = [2]\n",
    "#Pooling layers settings\n",
    "pool_size_options_2 = [2]\n",
    "#Penultimate dense layer settings\n",
    "nodes_options_2 = [45, 50]\n",
    "model_type_options_2 = ['Small_filters']\n",
    "\n",
    "#Get number of permutations of variable parameters\n",
    "permutations_2 = len(list(itertools.\n",
    "                        product(sample_weight_options_2, \n",
    "                                input_filter_count_options_2,\n",
    "                                input_kernel_size_options_2,\n",
    "                                input_strides_options_2,\n",
    "                                hidden_filters_options_2,\n",
    "                                hidden_kernel_size_options_2,\n",
    "                                hidden_strides_options_2,\n",
    "                                pool_size_options_2,\n",
    "                                nodes_options_2,\n",
    "                                model_type_options_2)))\n",
    "\n",
    "#Create dataframe of our first set of variable params\n",
    "variable_params_2 = pd.DataFrame(itertools.\n",
    "                               product(sample_weight_options_2, \n",
    "                                input_filter_count_options_2,\n",
    "                                input_kernel_size_options_2,\n",
    "                                input_strides_options_2,\n",
    "                                hidden_filters_options_2,\n",
    "                                hidden_kernel_size_options_2,\n",
    "                                hidden_strides_options_2,\n",
    "                                pool_size_options_2,\n",
    "                                nodes_options_2,\n",
    "                                model_type_options_2), \n",
    "                           columns=columns)\n",
    "\n",
    "#Duplicate each row of our locked params to match the number of variable\n",
    "#permutations we are going to run\n",
    "locked_temp = pd.DataFrame(np.repeat(locked_params.values, \n",
    "                                     permutations_1+permutations_2, \n",
    "                                     axis=0))\n",
    "locked_temp.columns = locked_params.columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Concat variable param grids together, and then on top of themselves\n",
    "#Nine times to get data properly formatted\n",
    "variable_params = pd.concat((variable_params, variable_params_2),\n",
    "                              ignore_index=True)\n",
    "variable_params = pd.concat((variable_params, \n",
    "                             variable_params,\n",
    "                             variable_params,\n",
    "                             variable_params,\n",
    "                             variable_params,\n",
    "                             variable_params,\n",
    "                             variable_params,\n",
    "                             variable_params,\n",
    "                             variable_params),\n",
    "                              ignore_index=True)\n",
    "\n",
    "#Join our locked and variable parameters\n",
    "test_df = locked_temp.join(variable_params)\n",
    "\n",
    "#Add our column for test accuracy\n",
    "test_df['val_acc'] = None\n",
    "\n",
    "#Resent NaNs to None so MNE can read\n",
    "test_df.replace(np.nan, None, inplace=True)\n",
    "\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059270a3",
   "metadata": {},
   "source": [
    "### Creating our function to iterate across our test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c2ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_grid_search(test_df, \n",
    "                    savefile):\n",
    "    \"\"\"A function to iterate across a test dataframe and fill in the score on\n",
    "    test data in the results column.\n",
    "    \n",
    "    All tests /rows must be at the individual subject level.\n",
    "    \n",
    "    Savefile is the filename to use in saving test dataframe to csv in data \n",
    "    directory each iteration.\"\"\"\n",
    "    for row in range(test_df.shape[0]):\n",
    "        #Load each sessions data into an MNE raw object\n",
    "        raw_dict = {}\n",
    "        for key, value in data_dict.items():\n",
    "            if 'sesh_1' in key:\n",
    "                raw_dict[key] = mne.io.RawArray(value.T, info, verbose=0)\n",
    "\n",
    "        #Filter data with bandpass. Note raw.filter applies in place\n",
    "        for key, value in raw_dict.items():\n",
    "            value.filter(l_freq=test_df.l_freq_filter[row], \n",
    "                         h_freq=test_df.h_freq_filter[row], \n",
    "                         method='fir', phase='zero', verbose=0)\n",
    "\n",
    "        #Create epoch object with our raw objects and events arrays\n",
    "        channels_to_keep = [ch for ch in ch_names if \n",
    "                            ch not in test_df.channels_to_drop[row]]\n",
    "        epoch_dict = {}\n",
    "        for key, value in raw_dict.items():\n",
    "            epoch_dict[key] = mne.Epochs(value, events=event_dict[key], \n",
    "                                        event_id=events_explained, \n",
    "                                        tmin=-3, tmax=test_df.tmax[row], \n",
    "                                        baseline=test_df.baseline_correction[row],\n",
    "                                        preload=True,\n",
    "                                        picks=channels_to_keep, verbose=0,\n",
    "                                        detrend=test_df.detrend[row],\n",
    "                                        reject=test_df.reject[row],\n",
    "                                        flat=test_df.flat[row],\n",
    "                                        reject_tmin=test_df.tmin[row],\n",
    "                                        reject_tmax=test_df.tmax[row])\n",
    "\n",
    "        #Skip creating projectors step to save compute time if not being\n",
    "        #applied in this iteration\n",
    "        if test_df.projectors_to_apply[row]:\n",
    "            #Create dictionary signal space projection vectors for each epoch\n",
    "            proj_dict = {}\n",
    "            for key, value in epoch_dict.items():\n",
    "                proj_dict[key] = mne.compute_proj_epochs(value, \n",
    "                                                         n_eeg=2, \n",
    "                                                         verbose=0)\n",
    "            #apply projectors\n",
    "            for key, value in epoch_dict.items():\n",
    "                value.add_proj(proj_dict[key][test_df.projectors_to_apply[row]], \n",
    "                               verbose=0)\n",
    "                value.apply_proj(verbose=0)\n",
    "\n",
    "        #Skip creating ICA components step to save compute time if not\n",
    "        #being applied in this iteration\n",
    "        if test_df.ica_to_exclude[row]:\n",
    "            #create and fit ICA object to epochs\n",
    "            for key, value in epoch_dict.items():\n",
    "                ica = mne.preprocessing.ICA(n_components=5, method='picard', \n",
    "                                            max_iter='auto', verbose=0)\n",
    "                ica.fit(value, verbose=0)\n",
    "                #Apply the ICA\n",
    "                ica.apply(value, exclude=test_df.ica_to_exclude[row],\n",
    "                         verbose=0)\n",
    "\n",
    "        #Resample the data at a new frequency, happens inplace\n",
    "        for key, value in epoch_dict.items():\n",
    "            value.resample(sfreq=test_df.selected_frequency[row])\n",
    "\n",
    "        #Extract and standard scale data from all non-dropped epochs\n",
    "        #Creates intermediate data dictionary\n",
    "        int_data_dict = {}\n",
    "        #Use robust sklearn scaler\n",
    "        if test_df.scaler[row] == 'robust':\n",
    "            mne_scaler = mne.decoding.Scaler(scalings='median')\n",
    "            for key, value in epoch_dict.items():\n",
    "                #with scalings=median implements sklearn robust scaler\n",
    "                int_data_dict[key] = (mne_scaler.\n",
    "                                      fit_transform(value.\n",
    "                                                    get_data(tmin=test_df.tmin[row], \n",
    "                                                             tmax=test_df.tmax[row])))\n",
    "        #No scaling option\n",
    "        if test_df.scaler[row] is None:\n",
    "            for key, value in epoch_dict.items():\n",
    "                int_data_dict[key] = value.get_data(tmin=test_df.tmin[row], \n",
    "                                                      tmax=test_df.tmax[row])\n",
    "\n",
    "        #Create updated dictionary of y values to reflect dropped epochs\n",
    "        int_y_dict = {}\n",
    "        for key, value in y_dict.items():\n",
    "            if 'sesh_1' in key:\n",
    "                temp_y_list = []\n",
    "                for i, epoch in enumerate(epoch_dict[key].drop_log):\n",
    "            #MNE drop log shows empty parens for epochs that were not dropped - \n",
    "            #these are the trials we are keeping in each iteration\n",
    "                    if epoch == ():\n",
    "                        temp_y_list.append(value[i])\n",
    "                int_y_dict[key] = temp_y_list\n",
    "\n",
    "        #Assemble final y dict with only trials in our current combo\n",
    "        #In each combo, coding 1st trial type to 0, 2nd trial type to 1\n",
    "        final_y_dict = {}\n",
    "        for key, value in int_y_dict.items():\n",
    "            temp_y_list = []\n",
    "            for y in value:\n",
    "                if y == test_df.trial_combo[row][0]:\n",
    "                    temp_y_list.append(0)\n",
    "                if y == test_df.trial_combo[row][1]:\n",
    "                    temp_y_list.append(1)\n",
    "            final_y_dict[key] = np.array(temp_y_list)\n",
    "\n",
    "        #Assemble data dict with only trials in our current combo\n",
    "        final_data_dict = {}\n",
    "        for key, value in int_data_dict.items():\n",
    "            index_list = []\n",
    "            for i, y in enumerate(int_y_dict[key]):\n",
    "                if (y == test_df.trial_combo[row][0] or \n",
    "                    y == test_df.trial_combo[row][1]):\n",
    "                    index_list.append(i)\n",
    "            final_data_dict[key] = value[index_list]\n",
    "\n",
    "        #Train test split the data and y for the subject of current row\n",
    "        for key, value in final_data_dict.items():\n",
    "            if test_df.subject[row] in key:\n",
    "                sub_X = value\n",
    "        for key, value in final_y_dict.items():\n",
    "            if test_df.subject[row] in key:\n",
    "                sub_y = value\n",
    "        (X_train, X_test, \n",
    "         y_train, y_test) = train_test_split(sub_X, \n",
    "                                             sub_y, \n",
    "                                             stratify=sub_y)\n",
    "        \n",
    "        #For subject, reset X and y in final dicts with train values,\n",
    "        #test values will be passed to model for validation\n",
    "        for key, value in final_data_dict.items():\n",
    "            if test_df.subject[row] in key:\n",
    "                final_data_dict[key] = X_train\n",
    "        for key, value in final_y_dict.items():\n",
    "            if test_df.subject[row] in key:\n",
    "                final_y_dict[key] = y_train\n",
    "        \n",
    "        #Concatenate all X and y values together into X_train and y_train\n",
    "        X_train = np.concatenate(([final_data_dict[key] for \n",
    "                                   key in final_data_dict.keys()]), axis=0)\n",
    "        y_train = np.concatenate(([final_y_dict[key] for \n",
    "                                   key in final_y_dict.keys()]))\n",
    "        \n",
    "        \n",
    "        #Create sample weight lists, higher weight for subject\n",
    "        sample_weights = []\n",
    "        for key, value in final_y_dict.items():\n",
    "            if test_df.subject[row] in key:\n",
    "                temp = [test_df.sample_weight[row]] * len(value)\n",
    "                sample_weights = sample_weights + temp\n",
    "            else:\n",
    "                temp = [1] * len(value)\n",
    "                sample_weights = sample_weights + temp\n",
    "        \n",
    "        #All data in y_test (our val data) is from the subject and\n",
    "        #assigned our test sample weight\n",
    "        val_sample_weights = [test_df.sample_weight[row]] * len(y_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Reshape all data tensors to feed into neural network\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0],\n",
    "                                       X_train.shape[2],\n",
    "                                       X_train.shape[1]))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0],\n",
    "                                     X_test.shape[2],\n",
    "                                     X_test.shape[1]))\n",
    "        y_train = np.reshape(y_train, \n",
    "                             len(y_train))\n",
    "        y_test = np.reshape(y_test, \n",
    "                            len(y_test))\n",
    "        sample_weights = np.reshape(sample_weights, \n",
    "                                    len(sample_weights))\n",
    "        val_sample_weights = np.reshape(val_sample_weights, \n",
    "                                        len(val_sample_weights))\n",
    "\n",
    "\n",
    "        #Suppress output\n",
    "        with io.capture_output() as captured:\n",
    "            #Model against our data and save the resulting scores\n",
    "            model = Sequential()\n",
    "            model.add(Conv1D(filters=test_df.input_filter_count[row], \n",
    "                             kernel_size=int(test_df.input_kernel_size[row]), \n",
    "                             strides=int(test_df.input_strides[row]), \n",
    "                             padding='same', \n",
    "                             activation='relu', \n",
    "                             input_shape=(X_train.shape[1], \n",
    "                                          X_train.shape[2])))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(MaxPooling1D(int(test_df.pool_size[row])))\n",
    "            model.add(Conv1D(filters=test_df.hidden_filters[row], \n",
    "                             kernel_size=int(test_df.hidden_kernel_size[row]), \n",
    "                             strides=int(test_df.hidden_strides[row]), \n",
    "                             padding='same', \n",
    "                             activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(MaxPooling1D(int(test_df.pool_size[row])))\n",
    "            model.add(GlobalAveragePooling1D())\n",
    "            model.add(Dense(test_df.nodes[row], activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        #Suppress output\n",
    "        with io.capture_output() as captured:\n",
    "            #compile & fit model\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  weighted_metrics=['accuracy'])\n",
    "        #Suppress output\n",
    "        with io.capture_output() as captured:\n",
    "            history = model.fit(x=X_train, \n",
    "                                y=y_train, \n",
    "                                sample_weight=sample_weights,\n",
    "                                batch_size=60, epochs=3, \n",
    "                                validation_data=(X_test, \n",
    "                                                 y_test,\n",
    "                                                 val_sample_weights), \n",
    "                                verbose=0, workers=8)\n",
    "\n",
    "        #Save the best val and training accuracy achieved by each model\n",
    "        test_df.at[row, 'val_acc'] = max(history.history['val_accuracy'])\n",
    "        test_df.at[row, 'train_acc'] = max(history.history['accuracy'])\n",
    "\n",
    "        test_df.to_csv(f'data/{savefile}.csv', index=False)\n",
    "        if row % 50 == 0:\n",
    "            print(f'Grid search complete through row {row} of {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26435b2",
   "metadata": {},
   "source": [
    "### Now let's run our tests. See the bottom of this notebook for the CNN gridsearch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c29017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete through row 0 of 1152\n",
      "Grid search complete through row 50 of 1152\n",
      "Grid search complete through row 100 of 1152\n",
      "Grid search complete through row 150 of 1152\n",
      "Grid search complete through row 200 of 1152\n",
      "Grid search complete through row 250 of 1152\n",
      "Grid search complete through row 300 of 1152\n",
      "Grid search complete through row 350 of 1152\n",
      "Grid search complete through row 400 of 1152\n",
      "Grid search complete through row 450 of 1152\n",
      "Grid search complete through row 500 of 1152\n",
      "Grid search complete through row 550 of 1152\n",
      "Grid search complete through row 600 of 1152\n",
      "Grid search complete through row 650 of 1152\n",
      "Grid search complete through row 700 of 1152\n",
      "Grid search complete through row 750 of 1152\n",
      "Grid search complete through row 800 of 1152\n",
      "Grid search complete through row 850 of 1152\n",
      "Grid search complete through row 900 of 1152\n",
      "Grid search complete through row 950 of 1152\n",
      "Grid search complete through row 1000 of 1152\n",
      "Grid search complete through row 1050 of 1152\n",
      "Grid search complete through row 1100 of 1152\n",
      "Grid search complete through row 1150 of 1152\n"
     ]
    }
   ],
   "source": [
    "#Run the test\n",
    "cnn_grid_search(test_df=test_df, \n",
    "                savefile='L1_CNN_gridsearch_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a5917e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial_combo</th>\n",
       "      <th>flat</th>\n",
       "      <th>reject</th>\n",
       "      <th>baseline_correction</th>\n",
       "      <th>detrend</th>\n",
       "      <th>ica_to_exclude</th>\n",
       "      <th>scaler</th>\n",
       "      <th>l_freq_filter</th>\n",
       "      <th>h_freq_filter</th>\n",
       "      <th>channels_to_drop</th>\n",
       "      <th>projectors_to_apply</th>\n",
       "      <th>selected_frequency</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>sample_weight</th>\n",
       "      <th>input_filter_count</th>\n",
       "      <th>input_kernel_size</th>\n",
       "      <th>input_strides</th>\n",
       "      <th>hidden_filters</th>\n",
       "      <th>hidden_kernel_size</th>\n",
       "      <th>hidden_strides</th>\n",
       "      <th>pool_size</th>\n",
       "      <th>nodes</th>\n",
       "      <th>model_type</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.555891</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.565495</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.528701</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.550955</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.519637</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.551515</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.530351</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.520767</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.561905</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>Large_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject trial_combo           flat        reject baseline_correction  \\\n",
       "1058   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1043   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1057   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1049   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1063   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1071   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1069   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1037   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1036   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1055   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "\n",
       "     detrend ica_to_exclude  scaler l_freq_filter h_freq_filter  \\\n",
       "1058    None           None  robust           1.0          40.0   \n",
       "1043    None           None  robust           1.0          40.0   \n",
       "1057    None           None  robust           1.0          40.0   \n",
       "1049    None           None  robust           1.0          40.0   \n",
       "1063    None           None  robust           1.0          40.0   \n",
       "1071    None           None  robust           1.0          40.0   \n",
       "1069    None           None  robust           1.0          40.0   \n",
       "1037    None           None  robust           1.0          40.0   \n",
       "1036    None           None  robust           1.0          40.0   \n",
       "1055    None           None  robust           1.0          40.0   \n",
       "\n",
       "                                       channels_to_drop projectors_to_apply  \\\n",
       "1058  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1043  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1057  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1049  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1063  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1071  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1069  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1037  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1036  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1055  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "\n",
       "     selected_frequency tmin tmax train_acc  sample_weight  \\\n",
       "1058                256    1  5.5  0.555891              3   \n",
       "1043                256    1  5.5  0.565495              2   \n",
       "1057                256    1  5.5  0.528701              3   \n",
       "1049                256    1  5.5  0.550955              2   \n",
       "1063                256    1  5.5  0.519637              3   \n",
       "1071                256    1  5.5  0.531915              3   \n",
       "1069                256    1  5.5  0.551515              3   \n",
       "1037                256    1  5.5  0.530351              2   \n",
       "1036                256    1  5.5  0.520767              2   \n",
       "1055                256    1  5.5  0.561905              2   \n",
       "\n",
       "      input_filter_count  input_kernel_size  input_strides  hidden_filters  \\\n",
       "1058                  20                 10              6              30   \n",
       "1043                  30                 10              6              30   \n",
       "1057                  20                 10              6              20   \n",
       "1049                  30                 15              6              20   \n",
       "1063                  20                 10              8              30   \n",
       "1071                  20                 15              8              30   \n",
       "1069                  20                 15              8              20   \n",
       "1037                  20                 15              8              20   \n",
       "1036                  20                 15              8              20   \n",
       "1055                  30                 15              8              30   \n",
       "\n",
       "      hidden_kernel_size  hidden_strides  pool_size  nodes     model_type  \\\n",
       "1058                   7               7          4     20  Large_filters   \n",
       "1043                   7               7          4     30  Large_filters   \n",
       "1057                   7               7          4     30  Large_filters   \n",
       "1049                   7               7          4     30  Large_filters   \n",
       "1063                   7               7          4     30  Large_filters   \n",
       "1071                   7               7          4     30  Large_filters   \n",
       "1069                   7               7          4     30  Large_filters   \n",
       "1037                   7               7          4     30  Large_filters   \n",
       "1036                   7               7          4     20  Large_filters   \n",
       "1055                   7               7          4     30  Large_filters   \n",
       "\n",
       "       val_acc  \n",
       "1058       1.0  \n",
       "1043  0.833333  \n",
       "1057  0.833333  \n",
       "1049  0.833333  \n",
       "1063  0.833333  \n",
       "1071  0.833333  \n",
       "1069  0.833333  \n",
       "1037  0.833333  \n",
       "1036  0.833333  \n",
       "1055  0.833333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#view our results per subject - large filters\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(test_df.loc[(test_df['subject'] == 'sub_L') & \n",
    "                        (test_df['model_type'] == 'Large_filters')].\n",
    "            sort_values('val_acc', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27f87d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>trial_combo</th>\n",
       "      <th>flat</th>\n",
       "      <th>reject</th>\n",
       "      <th>baseline_correction</th>\n",
       "      <th>detrend</th>\n",
       "      <th>ica_to_exclude</th>\n",
       "      <th>scaler</th>\n",
       "      <th>l_freq_filter</th>\n",
       "      <th>h_freq_filter</th>\n",
       "      <th>channels_to_drop</th>\n",
       "      <th>projectors_to_apply</th>\n",
       "      <th>selected_frequency</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>sample_weight</th>\n",
       "      <th>input_filter_count</th>\n",
       "      <th>input_kernel_size</th>\n",
       "      <th>input_strides</th>\n",
       "      <th>hidden_filters</th>\n",
       "      <th>hidden_kernel_size</th>\n",
       "      <th>hidden_strides</th>\n",
       "      <th>pool_size</th>\n",
       "      <th>nodes</th>\n",
       "      <th>model_type</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.504732</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.507886</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.50625</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.534375</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.536443</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.55102</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.48688</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.51895</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.51312</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>{'eeg': 29.5}</td>\n",
       "      <td>{'eeg': 100}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...</td>\n",
       "      <td>slice(1, 2, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.507886</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>Small_filters</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject trial_combo           flat        reject baseline_correction  \\\n",
       "1089   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1097   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1107   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1105   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1134   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1133   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1132   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1131   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1130   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "1088   sub_L      (1, 4)  {'eeg': 29.5}  {'eeg': 100}                None   \n",
       "\n",
       "     detrend ica_to_exclude  scaler l_freq_filter h_freq_filter  \\\n",
       "1089    None           None  robust           1.0          40.0   \n",
       "1097    None           None  robust           1.0          40.0   \n",
       "1107    None           None  robust           1.0          40.0   \n",
       "1105    None           None  robust           1.0          40.0   \n",
       "1134    None           None  robust           1.0          40.0   \n",
       "1133    None           None  robust           1.0          40.0   \n",
       "1132    None           None  robust           1.0          40.0   \n",
       "1131    None           None  robust           1.0          40.0   \n",
       "1130    None           None  robust           1.0          40.0   \n",
       "1088    None           None  robust           1.0          40.0   \n",
       "\n",
       "                                       channels_to_drop projectors_to_apply  \\\n",
       "1089  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1097  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1107  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1105  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1134  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1133  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1132  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1131  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1130  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "1088  ['AFz', 'F7', 'F8', 'T3', 'T4', 'P7', 'PO3', '...   slice(1, 2, None)   \n",
       "\n",
       "     selected_frequency tmin tmax train_acc  sample_weight  \\\n",
       "1089                256    1  5.5  0.504732              2   \n",
       "1097                256    1  5.5  0.507886              2   \n",
       "1107                256    1  5.5   0.50625              2   \n",
       "1105                256    1  5.5  0.534375              2   \n",
       "1134                256    1  5.5  0.536443              3   \n",
       "1133                256    1  5.5   0.55102              3   \n",
       "1132                256    1  5.5   0.48688              3   \n",
       "1131                256    1  5.5   0.51895              3   \n",
       "1130                256    1  5.5   0.51312              3   \n",
       "1088                256    1  5.5  0.507886              2   \n",
       "\n",
       "      input_filter_count  input_kernel_size  input_strides  hidden_filters  \\\n",
       "1089                  45                  3              1              45   \n",
       "1097                  45                  6              1              45   \n",
       "1107                  55                  3              1              55   \n",
       "1105                  55                  3              1              45   \n",
       "1134                  45                  6              2              55   \n",
       "1133                  45                  6              2              45   \n",
       "1132                  45                  6              2              45   \n",
       "1131                  45                  6              1              55   \n",
       "1130                  45                  6              1              55   \n",
       "1088                  45                  3              1              45   \n",
       "\n",
       "      hidden_kernel_size  hidden_strides  pool_size  nodes     model_type  \\\n",
       "1089                   3               2          2     50  Small_filters   \n",
       "1097                   3               2          2     50  Small_filters   \n",
       "1107                   3               2          2     50  Small_filters   \n",
       "1105                   3               2          2     50  Small_filters   \n",
       "1134                   3               2          2     45  Small_filters   \n",
       "1133                   3               2          2     50  Small_filters   \n",
       "1132                   3               2          2     45  Small_filters   \n",
       "1131                   3               2          2     50  Small_filters   \n",
       "1130                   3               2          2     45  Small_filters   \n",
       "1088                   3               2          2     45  Small_filters   \n",
       "\n",
       "       val_acc  \n",
       "1089  0.833333  \n",
       "1097  0.833333  \n",
       "1107  0.833333  \n",
       "1105  0.833333  \n",
       "1134  0.666667  \n",
       "1133  0.666667  \n",
       "1132  0.666667  \n",
       "1131  0.666667  \n",
       "1130  0.666667  \n",
       "1088  0.666667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#view our results per subject - small filters\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(test_df.loc[(test_df['subject'] == 'sub_L') & \n",
    "                        (test_df['model_type'] == 'Small_filters')].\n",
    "            sort_values('val_acc', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5658b0",
   "metadata": {},
   "source": [
    "**Based on the dataframe above, let's assemble our selections of final CNN models**\n",
    "\n",
    "Looked through the list of output manually to find models that were very similar on train and test accuracy and 75-80% accurate, in both the larger and smaller models category.\n",
    "\n",
    "In this case, train accuracy is the accuracy of the model against the entire dataset of session 1, less our small test split from the particular subject (since I fed it the whole dataset but added extra sample weight to our target candidate). My training accuracies as a result are generally quite low, and so I'm often moving forward with models that are only 55% accurate. Unsurprisingly, those models that are most broadly applicable tend to have a lower sample weight toward our target.\n",
    "\n",
    "There are often cases where they do much better on the test data (which is just the subject in question). For now, however, I am not bringing those models into my final ensemble model, but I am going to save them down into an index list, and if my ensemble isn't performing well, they could be a good group to add to the mix. I am not pulling the highest performing models, again to avoid overfitting, but am picking those with approx 75-80% val accuracy for that list. Unfurprisingly, these have a higher weight toward our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9248871",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_train_acc_models = [12, 121, \n",
    "                         181, 238, \n",
    "                         312, 351, \n",
    "                         411, 462, \n",
    "                         569, 592, \n",
    "                         658, 752,\n",
    "                         827, 832,\n",
    "                         925, 988,\n",
    "                         1072, 1151]\n",
    "high_val_acc_models = [7, 103,\n",
    "                      150, 216,\n",
    "                      295, 378,\n",
    "                      443, 483,\n",
    "                      515, 631,\n",
    "                      683, 735,\n",
    "                      823, 881,\n",
    "                      915, 974,\n",
    "                      1043, 1105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92225f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.iloc[high_train_acc_models].to_csv('data/CNN_models_most_general.csv',\n",
    "                                          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "452e7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.iloc[high_val_acc_models].to_csv('data/CNN_models_more_biased.csv',\n",
    "                                          index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
