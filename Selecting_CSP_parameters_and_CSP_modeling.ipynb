{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "83d60ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:54.936821Z",
     "start_time": "2022-07-15T04:16:38.839877Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Flatten, Conv1D, \n",
    "                                     MaxPooling1D, GlobalAveragePooling1D)\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1cf4f3",
   "metadata": {},
   "source": [
    "### Common Spatial Patterns (CSP)\n",
    "\n",
    "Next, we will implement and test CSP against our data to try and improve our predictive ability. In general, CSP is a signal processing technique (particularly for classification problems) in which multivariate signals (e.g., an EEG device with 30 electrodes, like we are using here) are separated into subcomponents which maximize the differences between the classes of signal.\n",
    "\n",
    "In practice, this will collapse our dataset for each test from an array of 30 channels x thousands of samples to just a few vector values. We will perform a gridsearch to find the ideal number of vectors for our problem, as well as whether a neural network or Linear Discriminant Analysis is the best modeling tool to make class predictions based on those CSP vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838b249",
   "metadata": {},
   "source": [
    "### First, let's ingest our data using the data ingester we built\n",
    "After running, this script will load several dictionaries into memory, as well as other needed objects:\n",
    "1. raw_dict - containing MNE raw objects with all the data\n",
    "2. event_dict - which indicates the sample number at which each stimulus was applied\n",
    "3. y_dict - which has the type of experiment conducted in each trial\n",
    "4. info -  file used to create MNE raw objects including channel names, type, and sampling frequency\n",
    "5. events_explained - dictionary which provides the names for each of the five trial types\n",
    "6. ch_names - list of all channel names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9beea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T01:17:12.272170Z",
     "start_time": "2022-07-14T01:16:29.919963Z"
    }
   },
   "outputs": [],
   "source": [
    "%run data_ingester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d77c34",
   "metadata": {},
   "source": [
    "### Let's find the best parameters to use to create the clearest differentiation using CSP\n",
    "\n",
    "In this grid search we'll be doing changing three major kinds of parameters to optimize our CSP settings: \n",
    "\n",
    "1. A smaller subset of the preprocessing options we tested using a CNN\n",
    "2. The CSP parameters to use in CSP feature extraction\n",
    "3. Whether an LDA or simple neural network makes more successful predictions based on CSP features\n",
    "\n",
    "Later, we will refine and iterate on the models we use to make predictions using this CSP data, but for now we will use a straightforward LDA and shallow neural network to test which sets of parameters lead to the most differentiable CSP features.\n",
    "\n",
    "Here is a full explanation of the CSP parameters and models we will be testing. For a full explanation of the preprocessing options, see the preprocessing options grid search notebook:\n",
    "\n",
    "1. Preprocessing options\n",
    "2. CSP parameters\n",
    "    - Number of CSP components to create (n_components) and transform data into\n",
    "    - Whether covariance matrices are created based on epochs concatenated together or on individual epochs and concatenated (cov_est)\n",
    "    - Whether a log transform is applied to standardize features\n",
    "3. Model to run through\n",
    "    - LDA\n",
    "    - Shallow neural network\n",
    "4. Which pairwise combination of trials to compare\n",
    "    - (1, 5) Word association vs imagining foot movement\n",
    "    - (1, 4) Word association vs imagining hand movement\n",
    "    - (2, 4) Mental subtraction vs imagining hand movement\n",
    "    - (1, 3) Word association vs mental navigation\n",
    "    \n",
    "**Also note that these CSP models will be created individually (i.e., looking at the data for only one participant in training and validation)**\n",
    "\n",
    "The grid search will save down the accuracy of an LDA and NN model on our test data from day 1 for each combination, and then we can select the highest-performing parameters to be used as the basis of the personalized L1 model for each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "003ae42b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:56.943456Z",
     "start_time": "2022-07-15T04:16:56.909545Z"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing parameters to gridsearch\n",
    "l_freq_filter_options = [None]\n",
    "h_freq_filter_options = [40]\n",
    "channels_to_drop_options = [['AFz', 'F7', 'F8']]\n",
    "baseline_correction_options = [None]\n",
    "projectors_to_apply_options = [slice(1)] #check that this generated best results\n",
    "selected_frequency_options = [256]\n",
    "tmin_options = [1] #Later start to avoid initialization of thought pattern\n",
    "tmax_options = [4.5]\n",
    "detrend_options = [None]\n",
    "reject_options = [{'eeg': 150}] #Customize per subject later\n",
    "flat_options = [{'eeg': 20}] #Customize per subject later\n",
    "ica_to_exclude_options = [None] #Incorporate later if helpful in other gridsearch\n",
    "scaler_options = ['robust', None] #Test no scaler for CSP\n",
    "#CSP parameters to gridsearch\n",
    "n_components_options = [4, 6, 8]\n",
    "cov_est_options = ['concat', 'epoch']\n",
    "log_options = [True, False]\n",
    "#Model to run through\n",
    "model_type_options = ['NN', 'LDA']\n",
    "#Combinations of trial types to compare\n",
    "trial_combo_options = [(1, 5), (1, 4), (2, 4), (1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "97585575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:56.959411Z",
     "start_time": "2022-07-15T04:16:56.949438Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create column names for test dataframe\n",
    "columns = ['l_freq_filter',\n",
    "           'h_freq_filter',\n",
    "           'channels_to_drop',\n",
    "           'baseline_correction',\n",
    "           'projectors_to_apply',\n",
    "           'selected_frequency',\n",
    "           'tmin',\n",
    "           'tmax',\n",
    "           'detrend',\n",
    "           'reject',\n",
    "           'flat',\n",
    "           'ica_to_exclude',\n",
    "           'scaler', \n",
    "           'n_components',\n",
    "           'cov_est', \n",
    "           'log', \n",
    "           'model_type',\n",
    "           'trial_combo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "8e345e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:56.991331Z",
     "start_time": "2022-07-15T04:16:56.966393Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create dataframe with all combinations of tests as rows\n",
    "test_df = pd.DataFrame(itertools.product(l_freq_filter_options, \n",
    "                                         h_freq_filter_options, \n",
    "                                         channels_to_drop_options, \n",
    "                                         baseline_correction_options, \n",
    "                                         projectors_to_apply_options, \n",
    "                                         selected_frequency_options,\n",
    "                                         tmin_options,\n",
    "                                         tmax_options, \n",
    "                                         detrend_options,\n",
    "                                         reject_options,\n",
    "                                         flat_options,\n",
    "                                         ica_to_exclude_options,\n",
    "                                         scaler_options, \n",
    "                                         n_components_options, \n",
    "                                         cov_est_options, \n",
    "                                         log_options, \n",
    "                                         model_type_options, \n",
    "                                         trial_combo_options), \n",
    "                      columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "2a17e8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:57.007286Z",
     "start_time": "2022-07-15T04:16:56.999306Z"
    }
   },
   "outputs": [],
   "source": [
    "#Append columns for each subject, where we will record results for each test\n",
    "subject_columns = ['sub_A', 'sub_C', 'sub_D', 'sub_E', 'sub_F', 'sub_G', \n",
    "                   'sub_H', 'sub_J', 'sub_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "95efa24b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:57.023241Z",
     "start_time": "2022-07-15T04:16:57.012269Z"
    }
   },
   "outputs": [],
   "source": [
    "#Add those combos to our test_df to save highest val accuracy achieved\n",
    "test_df = test_df.reindex(columns=columns + subject_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "091cee81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:57.055156Z",
     "start_time": "2022-07-15T04:16:57.028228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 27)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc48b6",
   "metadata": {},
   "source": [
    "### Let's test these options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "659fe4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete through row 0 of 192\n",
      "Grid search complete through row 20 of 192\n",
      "Grid search complete through row 40 of 192\n",
      "Grid search complete through row 60 of 192\n",
      "Grid search complete through row 80 of 192\n",
      "Grid search complete through row 100 of 192\n",
      "Grid search complete through row 120 of 192\n",
      "Grid search complete through row 140 of 192\n",
      "Grid search complete through row 160 of 192\n",
      "Grid search complete through row 180 of 192\n"
     ]
    }
   ],
   "source": [
    "for row in range(test_df.shape[0]):\n",
    "    #skip rows that have already been completed\n",
    "    if pd.isna(test_df.at[row, 'sub_A']):\n",
    "    \n",
    "        #Load each sessions data into an MNE raw object\n",
    "        raw_dict = {}\n",
    "        for key, value in data_dict.items():\n",
    "            raw_dict[key] = mne.io.RawArray(value.T, info, verbose=0)\n",
    "\n",
    "        #Filter data with bandpass. Note raw.filter applies in place\n",
    "        for key, value in raw_dict.items():\n",
    "            value.filter(l_freq=test_df.l_freq_filter[row], \n",
    "                         h_freq=test_df.h_freq_filter[row], \n",
    "                         method='fir', phase='zero', verbose=0)\n",
    "\n",
    "        #Create epoch object with our raw objects and events arrays\n",
    "        channels_to_keep = [ch for ch in ch_names if \n",
    "                            ch not in test_df.channels_to_drop[row]]\n",
    "        epoch_dict = {}\n",
    "        for key, value in raw_dict.items():\n",
    "            epoch_dict[key] = mne.Epochs(value, events=event_dict[key], \n",
    "                                        event_id=events_explained, \n",
    "                                        tmin=-3, tmax=test_df.tmax[row], \n",
    "                                        baseline=test_df.baseline_correction[row],\n",
    "                                        preload=True,\n",
    "                                        picks=channels_to_keep, verbose=0,\n",
    "                                        detrend=test_df.detrend[row],\n",
    "                                        reject=test_df.reject[row],\n",
    "                                        flat=test_df.flat[row],\n",
    "                                        reject_tmin=test_df.tmin[row],\n",
    "                                        reject_tmax=test_df.tmax[row])\n",
    "\n",
    "        #Skip creating projectors step to save compute time if not being\n",
    "        #applied in this iteration\n",
    "        if test_df.projectors_to_apply[row]:\n",
    "            #Create dictionary of top 5 signal space projection vectors for each epoch\n",
    "            proj_dict = {}\n",
    "            for key, value in epoch_dict.items():\n",
    "                proj_dict[key] = mne.compute_proj_epochs(value, n_eeg=1, verbose=0)\n",
    "            #apply projectors\n",
    "            for key, value in epoch_dict.items():\n",
    "                value.add_proj(proj_dict[key][test_df.projectors_to_apply[row]], \n",
    "                               verbose=0)\n",
    "                value.apply_proj(verbose=0)\n",
    "\n",
    "        #Skip creating ICA components step to save compute time if not\n",
    "        #being applied in this iteration\n",
    "        if test_df.ica_to_exclude[row]:\n",
    "            #create and fit ICA object to epochs\n",
    "            for key, value in epoch_dict.items():\n",
    "                ica = mne.preprocessing.ICA(n_components=5, method='picard', \n",
    "                                            max_iter='auto', verbose=0)\n",
    "                ica.fit(value, verbose=0)\n",
    "                #Apply the ICA\n",
    "                ica.apply(value, exclude=test_df.ica_to_exclude[row],\n",
    "                         verbose=0)\n",
    "\n",
    "        #Resample the data at a new frequency\n",
    "        for key, value in epoch_dict.items():\n",
    "            value.resample(sfreq=test_df.selected_frequency[row])\n",
    "\n",
    "        #Extract and standard scale data from all non-dropped epochs\n",
    "        #Creates intermediate data dictionary\n",
    "        int_data_dict = {}\n",
    "        #Use robust sklearn scaler\n",
    "        if test_df.scaler[row] == 'robust':\n",
    "            mne_scaler = mne.decoding.Scaler(scalings='median')\n",
    "            for key, value in epoch_dict.items():\n",
    "                #with scalings=median implements sklearn robust scaler\n",
    "                int_data_dict[key] = (mne_scaler.\n",
    "                                      fit_transform(value.\n",
    "                                                    get_data(tmin=test_df.tmin[row], \n",
    "                                                             tmax=test_df.tmax[row])))\n",
    "        #No scaling option\n",
    "        if test_df.scaler[row] is None:\n",
    "            for key, value in epoch_dict.items():\n",
    "                int_data_dict[key] = value.get_data(tmin=test_df.tmin[row], \n",
    "                                                      tmax=test_df.tmax[row])\n",
    "\n",
    "        #Create updated dictionary of y values to reflect dropped epochs\n",
    "        int_y_dict = {}\n",
    "        for key, value in y_dict.items():\n",
    "            temp_y_list = []\n",
    "            for i, epoch in enumerate(epoch_dict[key].drop_log):\n",
    "        #MNE drop log shows empty parens for epochs that were not dropped - \n",
    "        #these are the trials we are keeping in each iteration\n",
    "                if epoch == ():\n",
    "                    temp_y_list.append(value[i])\n",
    "            int_y_dict[key] = temp_y_list\n",
    "\n",
    "        #Assemble final y dict with only trials in our current combo\n",
    "        #In each combo, coding 1st trial type to 0, 2nd trial type to 1\n",
    "        final_y_dict = {}\n",
    "        for key, value in int_y_dict.items():\n",
    "            temp_y_list = []\n",
    "            for y in value:\n",
    "                if y == test_df.trial_combo[row][0]:\n",
    "                    temp_y_list.append(0)\n",
    "                if y == test_df.trial_combo[row][1]:\n",
    "                    temp_y_list.append(1)\n",
    "            final_y_dict[key] = np.array(temp_y_list)\n",
    "\n",
    "        #Assemble data dict with only trials in our current combo\n",
    "        final_data_dict = {}\n",
    "        for key, value in int_data_dict.items():\n",
    "            index_list = []\n",
    "            for i, y in enumerate(int_y_dict[key]):\n",
    "                if (y == test_df.trial_combo[row][0] or \n",
    "                    y == test_df.trial_combo[row][1]):\n",
    "                    index_list.append(i)\n",
    "            final_data_dict[key] = value[index_list]\n",
    "\n",
    "        #Create csp_dict of csp objects\n",
    "        csp_dict = {}\n",
    "        for key, value in epoch_dict.items():\n",
    "            #Only want to create csp objects for our train data - from session 1\n",
    "            if 'sesh_1' in key:\n",
    "                csp_dict[key] = mne.decoding.CSP(n_components=int(test_df.n_components[row]), \n",
    "                                                 cov_est=test_df.cov_est[row], \n",
    "                                                 log=bool(test_df.log[row]));\n",
    "\n",
    "        #Suppress output from this noisy function with no verbose option\n",
    "        with io.capture_output() as captured:\n",
    "        #Fit csp objects to training data from session 1        \n",
    "            for key, value in csp_dict.items():\n",
    "                #Try except to deal with iterations where fails to converge\n",
    "                try:\n",
    "                    value.fit(X=final_data_dict[key], \n",
    "                          y=final_y_dict[key]);\n",
    "                except:\n",
    "                    csp_dict[key] = 'CSP failed to converge'\n",
    "\n",
    "        #Use csp objects to transform and save resulting data\n",
    "        #Train test split sesh 1 data to avoid overfit on LDA\n",
    "        #Store train data in sesh 1, test in sesh 2\n",
    "        csp_data_dict = {}\n",
    "        for key, value in csp_dict.items():\n",
    "            #If except to deal with iterations where CSP fails to converge\n",
    "            if value == 'CSP failed to converge':\n",
    "                csp_data_dict[key] = 'CSP failed to converge'\n",
    "                key2 = key.replace('1', '2')\n",
    "                csp_data_dict[key2] = 'CSP failed to converge'\n",
    "            else:\n",
    "                X = value.transform(final_data_dict[key])\n",
    "                (X_train, X_test, \n",
    "                 y_train, y_test) = train_test_split(X, \n",
    "                                                     final_y_dict[key], \n",
    "                                                     stratify=final_y_dict[key])\n",
    "                csp_data_dict[key] = X_train\n",
    "                key2 = key.replace('1', '2')\n",
    "                csp_data_dict[key2] = X_test\n",
    "                final_y_dict[key] = y_train\n",
    "                final_y_dict[key2] = y_test\n",
    "\n",
    "\n",
    "        #Model against our data for each subject and save the resulting score\n",
    "        #First, LDA model\n",
    "        if test_df.model_type[row] == 'LDA':\n",
    "            #Base LDA objects on csp dict keys so only created for sesh 1\n",
    "            lda_dict = {}\n",
    "            for key in csp_dict.keys():\n",
    "                lda_dict[key] = LinearDiscriminantAnalysis()\n",
    "\n",
    "            #Fit LDA objects to training data from sesh 1\n",
    "            for key, value in lda_dict.items():\n",
    "                #If to deal with iterations where CSP fails to converge\n",
    "                if csp_dict[key] == 'CSP failed to converge':\n",
    "                    lda_dict[key] = 'CSP failed to converge' \n",
    "                else:\n",
    "                    value.fit(csp_data_dict[key], final_y_dict[key])\n",
    "\n",
    "            #Score on testing data from sesh 2 and save in test_df\n",
    "            for key, value in lda_dict.items():\n",
    "                #If to deal with iterations where CSP fails to converge\n",
    "                if value == 'CSP failed to converge':\n",
    "                    subject = key[:5]\n",
    "                    test_df.at[row, subject] = 'CSP failed to converge'\n",
    "                else:\n",
    "                    subject = key[:5]\n",
    "                    key2 = key.replace('1', '2')\n",
    "                    test_df.at[row, subject] = value.score(csp_data_dict[key2], \n",
    "                                                       final_y_dict[key2])\n",
    "\n",
    "        #Neural network\n",
    "        if test_df.model_type[row] == 'NN':\n",
    "            #Build model for each subject\n",
    "            #Base NN analysis on csp dict keys so only created for sesh 1\n",
    "            for key, value in csp_dict.items():      \n",
    "                #If to deal with iterations where CSP fails to converge\n",
    "                if csp_dict[key] == 'CSP failed to converge':\n",
    "                    subject = key[:5]\n",
    "                    test_df.at[row, subject] = 'CSP failed to converge'\n",
    "                    \n",
    "                else:\n",
    "                    #Build model\n",
    "                    model = Sequential()\n",
    "                    #inputs qre equal to n_components created via CSP\n",
    "                    model.add(Dense(test_df.n_components[row], \n",
    "                                       input_dim=test_df.n_components[row], \n",
    "                                       activation='relu'))\n",
    "                    model.add(Dropout(0.2))\n",
    "                    #Add hidden layer with half as many nodes as input\n",
    "                    model.add(Dense(test_df.n_components[row]/2, activation='relu'))\n",
    "                    model.add(Dropout(0.2))\n",
    "                    #Hidden layer with 1/4 as many nodes as input\n",
    "                    model.add(Dense(test_df.n_components[row]/4, activation='relu'))\n",
    "                    model.add(Dropout(0.2))\n",
    "                    #output layer\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "                    #Compile model\n",
    "                    model.compile(loss='binary_crossentropy', \n",
    "                                  optimizer='adam', \n",
    "                                  metrics=['acc'])\n",
    "\n",
    "                    #Fit model\n",
    "                    #Suppress output\n",
    "                    with io.capture_output() as captured:\n",
    "                        key2 = key.replace('1', '2')\n",
    "                        history = model.fit(csp_data_dict[key], final_y_dict[key], \n",
    "                                            validation_data=(csp_data_dict[key2], \n",
    "                                                             final_y_dict[key2]), \n",
    "                                            epochs=3, verbose=0)\n",
    "\n",
    "                    #Save validation accuracy into dataframe\n",
    "                    subject = key[:5]\n",
    "                    test_df.at[row, subject] = max(history.history['val_acc'])\n",
    "        test_df.to_csv('data/csp_grid_search.csv', index=False)\n",
    "        if row % 20 == 0:\n",
    "            print(f'Grid search complete through row {row} of {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa679438",
   "metadata": {},
   "source": [
    "**Let's look at the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "4691efba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_freq_filter</th>\n",
       "      <th>h_freq_filter</th>\n",
       "      <th>channels_to_drop</th>\n",
       "      <th>baseline_correction</th>\n",
       "      <th>projectors_to_apply</th>\n",
       "      <th>selected_frequency</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>detrend</th>\n",
       "      <th>reject</th>\n",
       "      <th>flat</th>\n",
       "      <th>ica_to_exclude</th>\n",
       "      <th>scaler</th>\n",
       "      <th>n_components</th>\n",
       "      <th>cov_est</th>\n",
       "      <th>log</th>\n",
       "      <th>model_type</th>\n",
       "      <th>trial_combo</th>\n",
       "      <th>sub_A</th>\n",
       "      <th>sub_C</th>\n",
       "      <th>sub_D</th>\n",
       "      <th>sub_E</th>\n",
       "      <th>sub_F</th>\n",
       "      <th>sub_G</th>\n",
       "      <th>sub_H</th>\n",
       "      <th>sub_J</th>\n",
       "      <th>sub_L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>4</td>\n",
       "      <td>epoch</td>\n",
       "      <td>False</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 5)</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>epoch</td>\n",
       "      <td>True</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>8</td>\n",
       "      <td>epoch</td>\n",
       "      <td>True</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>4</td>\n",
       "      <td>epoch</td>\n",
       "      <td>True</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>concat</td>\n",
       "      <td>False</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.55</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.875</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>concat</td>\n",
       "      <td>False</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 5)</td>\n",
       "      <td>0.65</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>0.6</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>8</td>\n",
       "      <td>concat</td>\n",
       "      <td>True</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>6</td>\n",
       "      <td>concat</td>\n",
       "      <td>True</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(2, 4)</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>robust</td>\n",
       "      <td>4</td>\n",
       "      <td>concat</td>\n",
       "      <td>False</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 5)</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>[AFz, F7, F8]</td>\n",
       "      <td>None</td>\n",
       "      <td>slice(None, 1, None)</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 150}</td>\n",
       "      <td>{'eeg': 20}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>concat</td>\n",
       "      <td>True</td>\n",
       "      <td>LDA</td>\n",
       "      <td>(1, 4)</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>CSP failed to converge</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    l_freq_filter  h_freq_filter channels_to_drop baseline_correction  \\\n",
       "28           None             40    [AFz, F7, F8]                None   \n",
       "117          None             40    [AFz, F7, F8]                None   \n",
       "85           None             40    [AFz, F7, F8]                None   \n",
       "21           None             40    [AFz, F7, F8]                None   \n",
       "109          None             40    [AFz, F7, F8]                None   \n",
       "108          None             40    [AFz, F7, F8]                None   \n",
       "69           None             40    [AFz, F7, F8]                None   \n",
       "38           None             40    [AFz, F7, F8]                None   \n",
       "12           None             40    [AFz, F7, F8]                None   \n",
       "101          None             40    [AFz, F7, F8]                None   \n",
       "\n",
       "      projectors_to_apply  selected_frequency  tmin  tmax detrend  \\\n",
       "28   slice(None, 1, None)                 256     1   4.5    None   \n",
       "117  slice(None, 1, None)                 256     1   4.5    None   \n",
       "85   slice(None, 1, None)                 256     1   4.5    None   \n",
       "21   slice(None, 1, None)                 256     1   4.5    None   \n",
       "109  slice(None, 1, None)                 256     1   4.5    None   \n",
       "108  slice(None, 1, None)                 256     1   4.5    None   \n",
       "69   slice(None, 1, None)                 256     1   4.5    None   \n",
       "38   slice(None, 1, None)                 256     1   4.5    None   \n",
       "12   slice(None, 1, None)                 256     1   4.5    None   \n",
       "101  slice(None, 1, None)                 256     1   4.5    None   \n",
       "\n",
       "           reject         flat ica_to_exclude  scaler  n_components cov_est  \\\n",
       "28   {'eeg': 150}  {'eeg': 20}           None  robust             4   epoch   \n",
       "117  {'eeg': 150}  {'eeg': 20}           None    None             4   epoch   \n",
       "85   {'eeg': 150}  {'eeg': 20}           None  robust             8   epoch   \n",
       "21   {'eeg': 150}  {'eeg': 20}           None  robust             4   epoch   \n",
       "109  {'eeg': 150}  {'eeg': 20}           None    None             4  concat   \n",
       "108  {'eeg': 150}  {'eeg': 20}           None    None             4  concat   \n",
       "69   {'eeg': 150}  {'eeg': 20}           None  robust             8  concat   \n",
       "38   {'eeg': 150}  {'eeg': 20}           None  robust             6  concat   \n",
       "12   {'eeg': 150}  {'eeg': 20}           None  robust             4  concat   \n",
       "101  {'eeg': 150}  {'eeg': 20}           None    None             4  concat   \n",
       "\n",
       "       log model_type trial_combo                   sub_A  \\\n",
       "28   False        LDA      (1, 5)                     0.6   \n",
       "117   True        LDA      (1, 4)                    0.35   \n",
       "85    True        LDA      (1, 4)                    0.65   \n",
       "21    True        LDA      (1, 4)                     0.5   \n",
       "109  False        LDA      (1, 4)                    0.55   \n",
       "108  False        LDA      (1, 5)                    0.65   \n",
       "69    True        LDA      (1, 4)                    0.65   \n",
       "38    True        LDA      (2, 4)                    0.75   \n",
       "12   False        LDA      (1, 5)                    0.55   \n",
       "101   True        LDA      (1, 4)  CSP failed to converge   \n",
       "\n",
       "                      sub_C                   sub_D                   sub_E  \\\n",
       "28                 0.538462                0.647059                0.588235   \n",
       "117                0.272727  CSP failed to converge  CSP failed to converge   \n",
       "85                 0.454545                    0.75                  0.8125   \n",
       "21                      0.5                0.705882                0.588235   \n",
       "109  CSP failed to converge  CSP failed to converge                  0.5625   \n",
       "108  CSP failed to converge                     0.6  CSP failed to converge   \n",
       "69                 0.454545                  0.6875                  0.8125   \n",
       "38                 0.545455                  0.9375                0.647059   \n",
       "12                 0.642857                0.764706                0.555556   \n",
       "101                0.363636                  0.8125                   0.875   \n",
       "\n",
       "                      sub_F                   sub_G                   sub_H  \\\n",
       "28                     0.75                    0.75                0.454545   \n",
       "117  CSP failed to converge  CSP failed to converge  CSP failed to converge   \n",
       "85                     0.85                    0.95                0.555556   \n",
       "21                     0.95                     0.6                     0.6   \n",
       "109  CSP failed to converge                    0.65                   0.875   \n",
       "108                    0.75                     0.5                0.333333   \n",
       "69                      0.9                     0.8                0.666667   \n",
       "38                      0.8                     0.7                     0.7   \n",
       "12                      0.7                     0.8                     0.5   \n",
       "101                     0.9                    0.75  CSP failed to converge   \n",
       "\n",
       "                      sub_J sub_L  \n",
       "28                 0.538462   1.0  \n",
       "117                0.666667   1.0  \n",
       "85                 0.666667   1.0  \n",
       "21                 0.571429  0.95  \n",
       "109  CSP failed to converge  0.95  \n",
       "108                0.454545  0.95  \n",
       "69                      0.5  0.95  \n",
       "38                 0.416667  0.95  \n",
       "12                 0.615385  0.95  \n",
       "101                0.583333  0.95  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Temporarily change pandas settings to see all columns\n",
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    #Change this code to match the subject of interest\n",
    "    display(test_df[test_df['sub_L'].apply(type) != str].\n",
    "            sort_values('sub_L', ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dce87d",
   "metadata": {},
   "source": [
    "### Identifying per subject ideal individual CSP model settings\n",
    "\n",
    "We'll be using these models based on CSP covariance matrices as one of our level 1 models in our final ensemble model. We have a bit more testing to do on those level 1 models though (notably individualizing the parameters to drop epochs), so let's examine our results thus far and extract the parameters we want to lock in for each subject. There are a few patterns here in the data:\n",
    "\n",
    "- Most subjects have a particular trial combo that seems to be most differentiable for them\n",
    "    - Not the same combination for every subject\n",
    "    - This is likely influenced by the nature and area of their central nervous system injury\n",
    "    - This is the most important parameter to improve modeling for each participant\n",
    "- Most subjects clearly have a number of CSP components that outperform the others, but that number varies\n",
    "- The robust scaler is uniformly better than no scaling of the data\n",
    "    - CSP often fails to fit without scaled data\n",
    "    - As such not saving down as an individual parameter - will just scale in all models\n",
    "- The cov_est setting does not seems to matter much, but will take the highest performing setting for each person\n",
    "- Log transforming appears to generally have a small impact on the CSP fit as well\n",
    "- LDA seems to perform better than the neural network for all but a few subjects. For those subjects where the LDA doesn't perform better than NN, it is always quite close, and those are the subjects where both model types are fairly bad. I'll plan to take the parameters from the highest performing model and then run both NN and LDA models on the CSP data to get a broader perspective.\n",
    "\n",
    "We'll save the remaining, variable highest performing parameters for each individual to be used going forward as the core of their individual L1 model. The parameters we are saving are:\n",
    "\n",
    "1. Trial combo\n",
    "3. Covariance estimate method\n",
    "4. Whether log transformed\n",
    "5. Number of csp components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561de176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to make data extractable from dataframe once opened:\n",
    "#First need to open the data with read_csv\n",
    "\n",
    "sub_A_list = []\n",
    "for entry in test_df.sub_A:\n",
    "    try:\n",
    "        sub_A_list.append(float(entry))\n",
    "    except:\n",
    "        sub_A_list.append(entry)\n",
    "test_df['sub_A'] = sub_A_list\n",
    "\n",
    "sub_C_list = []\n",
    "for entry in test_df.sub_C:\n",
    "    try:\n",
    "        sub_C_list.append(float(entry))\n",
    "    except:\n",
    "        sub_C_list.append(entry)\n",
    "        \n",
    "test_df['sub_C'] = sub_C_list\n",
    "\n",
    "sub_D_list = []\n",
    "for entry in test_df.sub_D:\n",
    "    try:\n",
    "        sub_D_list.append(float(entry))\n",
    "    except:\n",
    "        sub_D_list.append(entry)\n",
    "test_df['sub_D'] = sub_D_list\n",
    "\n",
    "sub_E_list = []\n",
    "for entry in test_df.sub_E:\n",
    "    try:\n",
    "        sub_E_list.append(float(entry))\n",
    "    except:\n",
    "        sub_E_list.append(entry)\n",
    "test_df['sub_E'] = sub_E_list\n",
    "\n",
    "sub_F_list = []\n",
    "for entry in test_df.sub_F:\n",
    "    try:\n",
    "        sub_F_list.append(float(entry))\n",
    "    except:\n",
    "        sub_F_list.append(entry)\n",
    "test_df['sub_F'] = sub_F_list\n",
    "\n",
    "sub_G_list = []\n",
    "for entry in test_df.sub_G:\n",
    "    try:\n",
    "        sub_G_list.append(float(entry))\n",
    "    except:\n",
    "        sub_G_list.append(entry)\n",
    "test_df['sub_G'] = sub_G_list\n",
    "\n",
    "sub_H_list = []\n",
    "for entry in test_df.sub_H:\n",
    "    try:\n",
    "        sub_H_list.append(float(entry))\n",
    "    except:\n",
    "        sub_H_list.append(entry)\n",
    "test_df['sub_H'] = sub_H_list\n",
    "\n",
    "sub_J_list = []\n",
    "for entry in test_df.sub_J:\n",
    "    try:\n",
    "        sub_J_list.append(float(entry))\n",
    "    except:\n",
    "        sub_J_list.append(entry)\n",
    "test_df['sub_J'] = sub_J_list\n",
    "\n",
    "sub_L_list = []\n",
    "for entry in test_df.sub_L:\n",
    "    try:\n",
    "        sub_L_list.append(float(entry))\n",
    "    except:\n",
    "        sub_L_list.append(entry)\n",
    "test_df['sub_L'] = sub_L_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6d522d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_save = ['n_components',\n",
    "                  'cov_est', \n",
    "                  'log', \n",
    "                  'trial_combo']\n",
    "\n",
    "#Create function to save the info we want for a given subject\n",
    "def pull_parameters(sub):\n",
    "    #identify the highest score achieved for this subject\n",
    "    sub_max = max(test_df[test_df[sub].apply(type) != str][sub])\n",
    "    #Get the index in the dataframe where that occurred\n",
    "    #In case of tie, returns first row\n",
    "    [max_index] = test_df[test_df[sub] == sub_max].head(1).index\n",
    "    #Save the params we want plus the subject from that best row\n",
    "    temp_df = pd.DataFrame(test_df.loc[max_index, params_to_save + [sub]]).T\n",
    "    temp_df[sub] = sub\n",
    "    #Duplicate the row 25 times to join with our 25 epoch dropping combos\n",
    "    return pd.concat([temp_df]*25, ignore_index=True)\n",
    "\n",
    "#Use function to create the beginning of next test dfs for each subject\n",
    "sub_A_csp_df = pull_parameters('sub_A')\n",
    "sub_C_csp_df = pull_parameters('sub_C')\n",
    "sub_D_csp_df = pull_parameters('sub_D')\n",
    "sub_E_csp_df = pull_parameters('sub_E')\n",
    "sub_F_csp_df = pull_parameters('sub_F')\n",
    "sub_G_csp_df = pull_parameters('sub_G')\n",
    "sub_H_csp_df = pull_parameters('sub_H')\n",
    "sub_J_csp_df = pull_parameters('sub_J')\n",
    "sub_L_csp_df = pull_parameters('sub_L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "3f13c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_C_csp_df = pull_parameters('sub_D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39dd191",
   "metadata": {},
   "source": [
    "### Identifying per-subject epoch rejection criteria\n",
    "\n",
    "The shapes of each individual's EEG recordings look very different - some have far more dramatic amplitude changes than others. The algorithm we are using to drop epochs is based on setting minimum and maximum amplitude differences between peaks and valleys in the data, so we need to personalize those settings if we want to be able to drop bad epochs in each individual study participants data.\n",
    "\n",
    "We will quantify the percentage of epochs that are dropped from each experiment session with a range of rejection criteria, and then experiment with how dropping increasing percentages of epochs impacts our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "cae6867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of epoch drop filters\n",
    "reject_options_1 = [None]\n",
    "reject_options_2 = [None, \n",
    "                  {'eeg': 40}, {'eeg': 50}, {'eeg': 60}, \n",
    "                  {'eeg': 70}, {'eeg': 80}, {'eeg': 90}, \n",
    "                  {'eeg': 105}, {'eeg': 140}, {'eeg': 175},\n",
    "                   {'eeg': 135}, {'eeg': 145}, {'eeg': 150},\n",
    "                   {'eeg': 155}, {'eeg': 160}, {'eeg': 167},\n",
    "                   {'eeg': 146}, {'eeg': 147}, \n",
    "                   {'eeg': 130}, {'eeg': 125}, {'eeg': 120},\n",
    "                   {'eeg': 115}, {'eeg': 110},\n",
    "                   {'eeg': 180}, {'eeg': 185}, {'eeg': 190},\n",
    "                   {'eeg': 143}, {'eeg': 110}, {'eeg': 115},\n",
    "                   {'eeg': 120}, {'eeg': 125}, {'eeg': 130},\n",
    "                   {'eeg': 83}, {'eeg': 86}, {'eeg': 78}, \n",
    "                   {'eeg': 100}, {'eeg': 95}]\n",
    "flat_options_1 = [None]\n",
    "flat_options_2 = [None, \n",
    "                {'eeg': 3}, {'eeg': 6}, {'eeg': 9}, \n",
    "                {'eeg': 11}, {'eeg': 13}, {'eeg': 15}, \n",
    "                {'eeg': 17}, {'eeg': 19}, {'eeg': 21},\n",
    "                 {'eeg': 25}, {'eeg': 30}, {'eeg': 35},\n",
    "                 {'eeg': 36}, {'eeg': 31}, {'eeg': 33}, \n",
    "                 {'eeg': 34}, {'eeg': 30.5}, {'eeg': 31.5},\n",
    "                 {'eeg': 16}, {'eeg': 18}, {'eeg': 20},\n",
    "                 {'eeg': 17.3}, {'eeg': 17.6}, {'eeg': 18.5},\n",
    "                 {'eeg': 18.75}, {'eeg': 19.5}, {'eeg': 20.5},\n",
    "                 {'eeg': 22}, {'eeg': 23}, {'eeg': 24}, \n",
    "                 {'eeg': 19.75},\n",
    "                 {'eeg': 24.5}, {'eeg': 26}, {'eeg': 27},\n",
    "                 {'eeg': 28}, {'eeg': 29}, \n",
    "                 {'eeg': 25.5}, {'eeg': 29.5}, {'eeg': 26.5}]\n",
    "\n",
    "#Create dataframe - tests flat & spiky filters independently\n",
    "rejection_settings_df = pd.concat((pd.DataFrame(itertools.product(reject_options_1,\n",
    "                                                                 flat_options_2), \n",
    "                                               columns=['reject', 'flat']),\n",
    "                                  pd.DataFrame(itertools.product(reject_options_2,\n",
    "                                                                 flat_options_1), \n",
    "                                               columns=['reject', 'flat'])),\n",
    "                                 ignore_index=True)\n",
    "\n",
    "#reindex\n",
    "rejection_settings_df = rejection_settings_df.reindex(columns=['reject', 'flat'] + \n",
    "                                                      list(csp_dict.keys()))\n",
    "\n",
    "#Compute percentage of trials dropped for each setting\n",
    "for row in range(rejection_settings_df.shape[0]): \n",
    "    epoch_dict = {}\n",
    "    for key, value in raw_dict.items():\n",
    "        epoch_dict[key] = mne.Epochs(value, events=event_dict[key], \n",
    "                                     event_id=events_explained, \n",
    "                                     tmin=-3, tmax=4.5, \n",
    "                                     baseline=None,\n",
    "                                     preload=True,\n",
    "                                     picks=channels_to_keep, verbose=0,\n",
    "                                     reject=rejection_settings_df.reject[row],\n",
    "                                     flat=rejection_settings_df.flat[row],\n",
    "                                     reject_tmin=1,\n",
    "                                     reject_tmax=4.5)\n",
    "    perc_trials_dropped_dict = {}\n",
    "    for key, value in y_dict.items():\n",
    "        dropped = value.shape[0] - epoch_dict[key].get_data().shape[0]\n",
    "        drop_percentage = dropped / value.shape[0]\n",
    "        rejection_settings_df.at[row, key] = drop_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "1bb6ec14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reject</th>\n",
       "      <th>flat</th>\n",
       "      <th>sub_L_sesh_1</th>\n",
       "      <th>sub_L_sesh_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 25}</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 24}</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 24.5}</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 25.5}</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 26}</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 26.5}</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 27}</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>None</td>\n",
       "      <td>{'eeg': 28}</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reject           flat  sub_L_sesh_1  sub_L_sesh_2\n",
       "10   None    {'eeg': 25}         0.015         0.275\n",
       "30   None    {'eeg': 24}         0.015         0.205\n",
       "32   None  {'eeg': 24.5}         0.015         0.255\n",
       "37   None  {'eeg': 25.5}         0.025         0.310\n",
       "33   None    {'eeg': 26}         0.040         0.360\n",
       "39   None  {'eeg': 26.5}         0.075         0.405\n",
       "34   None    {'eeg': 27}         0.110         0.425\n",
       "35   None    {'eeg': 28}         0.165         0.550"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PICK UP WORK HERE, DOUBLE CHECK MY TABLE AND SWAP ANY FROM SESH 2 TO SESH 1 IF NEEDED\n",
    "#view results\n",
    "(rejection_settings_df[['reject', 'flat', \n",
    "                        'sub_L_sesh_1']].\n",
    " loc[(rejection_settings_df['sub_L_sesh_1'] < 0.2) & \n",
    "     (rejection_settings_df['sub_L_sesh_1'] > 0) &\n",
    " (rejection_settings_df['reject'].isna())].sort_values(['sub_L_sesh_1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758d268",
   "metadata": {},
   "source": [
    "### Testing custom drop settings per subject with CSP model\n",
    "\n",
    "For each subject, we will set out to test how our highest performing CSP model for that individual performs with rejection settings designed to drop the following percentages of epochs. For each approximate drop percentage we are targeting, we will use both min and max peak to trough settings to drop epochs that are too flat and too spiky at those percentage (checked via grid search of combinations.\n",
    "\n",
    "- None (keep all epochs)\n",
    "- Drop 2%\n",
    "- Drop 4%\n",
    "- Drop 8%\n",
    "- Drop 12%\n",
    "\n",
    "The setting that results in the closest drop % to our targets in session 1 is what we will select. There is likely to be relatively wide variability in session 2 (our test data) compared to session 1, so there may be some subjects who have far more or far fewer epochs dropped in session 2. To avoid this in a production BCI device, it needs to either be trained on enough data to be resilient to inter-day swings in EEG signal, or be frequently rebiased or retrained in short calibration sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb7a7a",
   "metadata": {},
   "source": [
    "**Creating list of drop criteria we want to test for each individual**\n",
    "Based on the results of our grid search, let's manually assemble the list of settings we want to test for each individual that drop roughly the percentage of trials we are planning to drop.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9830c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want this to be a dictionary with keys equal to sub_A_flat, sub_A_reject, and then the list of values\n",
    "\n",
    "sub_A_flat_options = [None, {'eeg': 31.5}, {'eeg': 33}, {'eeg': 34}, {'eeg': 35}]\n",
    "sub_A_reject_options = [None, {'eeg': 167}, {'eeg': 155}, {'eeg': 150}, {'eeg': 143}]\n",
    "sub_C_flat_options = [None, {'eeg': 16}, {'eeg': 17.6}, {'eeg': 18.5}, {'eeg': 20}]\n",
    "sub_C_reject_options = [None, {'eeg': 160}, {'eeg': 145}, {'eeg': 130}, {'eeg': 125}]\n",
    "sub_D_flat_options = [None, {'eeg': 18.75}, {'eeg': 19.5}, {'eeg': 20.5}, {'eeg': 21}]\n",
    "sub_D_reject_options = [None, {'eeg': 190}, {'eeg': 185}, {'eeg': 150}, {'eeg': 120}]\n",
    "sub_E_flat_options = [None, {'eeg': 19}, {'eeg': 19.75}, {'eeg': 21}, {'eeg': 22}]\n",
    "sub_E_reject_options = [None, {'eeg': 150}, {'eeg': 143}, {'eeg': 125}, {'eeg': 105}]\n",
    "sub_F_flat_options = [None, {'eeg': 24}, {'eeg': 24.5}, {'eeg': 26}, {'eeg': 27}]\n",
    "sub_F_reject_options = [None, {'eeg': 90}, {'eeg': 86}, {'eeg': 82}, {'eeg': 78}]\n",
    "sub_G_flat_options = [None, {'eeg': 26}, {'eeg': 27}, {'eeg': 29}, {'eeg': 30}]\n",
    "sub_G_reject_options = [None, {'eeg': 160}, {'eeg': 150}, {'eeg': 100}, {'eeg': 80}]\n",
    "sub_H_flat_options = [None, {'eeg': 18}, {'eeg': 18.75}, {'eeg': 19.5}, {'eeg': 20}]\n",
    "sub_H_reject_options = [None, {'eeg': 185}, {'eeg': 175}, {'eeg': 167}, {'eeg': 150}]\n",
    "sub_J_flat_options = [None, {'eeg': 17.3}, {'eeg': 18}, {'eeg': 18.75}, {'eeg': 19.75}]\n",
    "sub_J_reject_options = [None, {'eeg': 155}, {'eeg': 143}, {'eeg': 130}, {'eeg': 105}]\n",
    "sub_L_flat_options = [None, {'eeg': 24}, {'eeg': 26}, {'eeg': 26.5}, {'eeg': 27}]\n",
    "sub_L_reject_options = [None, {'eeg': 100}, {'eeg': 90}, {'eeg': 83}, {'eeg': 78}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe19d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26ef16fe",
   "metadata": {},
   "source": [
    "### Model refinement\n",
    "\n",
    "Ensemble model testing - later\n",
    "- Testing LDA on individual model vs NN on individual model vs NN on heigher sample weighted but all data ingested model\n",
    "\n",
    "**I think if transform into is CSP space it will yield data in a shape that I should still feed it into a CNN - don't think that really belongs in this test set. Do that separately.**\n",
    "\n",
    "**Grid search for reject and flat settings to maintain 90%, 75%, 60% of epochs for each trial participant, right now we're dropping way more with some participants**\n",
    "\n",
    "Grid search sending in a shorter period of data into the CSP\n",
    "\n",
    "Grid search parameters for LDA if it is performing better than simple NN\n",
    "\n",
    "Test pulling more time shifted samples\n",
    "\n",
    "5. Resampling our training data (e.g., including -0.3 to 4.9, 0. to 5.2, and 0.3 to 5.5) to give our model more data to train on\n",
    "\n",
    "When doing the individual models, I can set the rejection criteria for epochs to match the individual in question better - if I have time.\n",
    "\n",
    "Read this article for other preprocessing ideas after getting dropping epochs set up: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5915520/\n",
    "\n",
    "Another thing I could play around with:\n",
    "\n",
    "- EEGLib\n",
    "    - https://www.sciencedirect.com/science/article/pii/S2352711021000753\n",
    "    - Primarily used for feature extraction after the data has been processed, but it does have some preprocessing capability\n",
    "    - Appears to be written to allow visual inspection of data and then creation of features based on the selected point - certainly worth investigating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
