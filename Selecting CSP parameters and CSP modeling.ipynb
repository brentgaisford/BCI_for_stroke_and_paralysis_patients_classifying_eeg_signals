{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d60ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:54.936821Z",
     "start_time": "2022-07-15T04:16:38.839877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Flatten, Conv1D, \n",
    "                                     MaxPooling1D, GlobalAveragePooling1D)\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1cf4f3",
   "metadata": {},
   "source": [
    "### Common Spatial Patterns (CSP)\n",
    "\n",
    "Next, we will implement and test CSP against our data to try and improve our predictive ability. In general, CSP is a signal processing technique (particularly for classification problems) in which multivariate signals (e.g., an EEG device with 30 electrodes, like we are using here) are separated into subcomponents which maximize the differences between the classes of signal.\n",
    "\n",
    "In practice, this will collapse our dataset for each test from an array of 30 channels x thousands of samples to just a few vector values. We will perform a gridsearch to find the ideal number of vectors for our problem, as well as whether a neural network or Linear Discriminant Analysis is the best modeling tool to make class predictions based on those CSP vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838b249",
   "metadata": {},
   "source": [
    "### First, let's ingest our data using the data ingester we built\n",
    "After running, this script will load several dictionaries into memory, as well as other needed objects:\n",
    "1. raw_dict - containing MNE raw objects with all the data\n",
    "2. event_dict - which indicates the sample number at which each stimulus was applied\n",
    "3. y_dict - which has the type of experiment conducted in each trial\n",
    "4. info -  file used to create MNE raw objects including channel names, type, and sampling frequency\n",
    "5. events_explained - dictionary which provides the names for each of the five trial types\n",
    "6. ch_names - list of all channel names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9beea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T01:17:12.272170Z",
     "start_time": "2022-07-14T01:16:29.919963Z"
    }
   },
   "outputs": [],
   "source": [
    "%run data_ingester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d77c34",
   "metadata": {},
   "source": [
    "### Let's find the best parameters to use to create the clearest differentiation using CSP\n",
    "\n",
    "In this grid search we'll be doing changing three major kinds of parameters to optimize our CSP settings: \n",
    "\n",
    "1. A smaller subset of the preprocessing options we tested using a CNN\n",
    "2. The CSP parameters to use in CSP feature extraction\n",
    "3. Whether an LDA or simple neural network makes more successful predictions based on CSP features\n",
    "\n",
    "Later, we will refine and iterate on the models we use to make predictions using this CSP data, but for now we will use a straightforward LDA and shallow neural network to test which sets of parameters lead to the most differentiable CSP features.\n",
    "\n",
    "Here is a full explanation of the CSP parameters and models we will be testing. For a full explanation of the preprocessing options, see the preprocessing options grid search notebook:\n",
    "\n",
    "1. Preprocessing options\n",
    "2. CSP parameters\n",
    "    - Number of CSP components to create (n_components) and transform data into\n",
    "    - Whether covariance matrices are created based on epochs concatenated together or on individual epochs and concatenated (cov_est)\n",
    "    - Whether a log transform is applied to standardize features\n",
    "3. Model to run through\n",
    "    - LDA\n",
    "    - Shallow neural network\n",
    "4. Which pairwise combination of trials to compare\n",
    "    - (1, 5) Word association vs imagining foot movement\n",
    "    - (1, 4) Word association vs imagining hand movement\n",
    "    - (2, 4) Mental subtraction vs imagining hand movement\n",
    "    - (1, 3) Word association vs mental navigation\n",
    "    \n",
    "**Also note that these CSP models will be created individually (i.e., looking at the data for only one participant in training and validation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003ae42b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:56.943456Z",
     "start_time": "2022-07-15T04:16:56.909545Z"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing parameters to gridsearch\n",
    "l_freq_filter_options = [None]\n",
    "h_freq_filter_options = [40]\n",
    "channels_to_drop_options = [['AFz', 'F7', 'F8']]\n",
    "baseline_correction_options = [None]\n",
    "projectors_to_apply_options = [slice(1)] #check that this generated best results\n",
    "selected_frequency_options = [256]\n",
    "tmin_options = [1] #Later start to avoid initialization of thought pattern\n",
    "tmax_options = [4.5]\n",
    "detrend_options = [None]\n",
    "reject_options = [{'eeg': 150}] #Customize per subject later\n",
    "flat_options = [{'eeg': 20}] #Customize per subject later\n",
    "ica_to_exclude_options = [None] #Incorporate later if helpful in other gridsearch\n",
    "scaler_options = ['robust', None] #Test no scaler for CSP\n",
    "#CSP parameters to gridsearch\n",
    "n_components_options = [4, 12, 24]\n",
    "cov_est_options = ['concat', 'epoch']\n",
    "log_options = [True, False]\n",
    "#Model to run through\n",
    "model_type_options = ['NN', 'LDA']\n",
    "#Combinations of trial types to compare\n",
    "trial_combo_options = [(1, 5), (1, 4), (2, 4), (1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97585575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:56.959411Z",
     "start_time": "2022-07-15T04:16:56.949438Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create column names for test dataframe\n",
    "columns = ['l_freq_filter',\n",
    "           'h_freq_filter',\n",
    "           'channels_to_drop',\n",
    "           'baseline_correction',\n",
    "           'projectors_to_apply',\n",
    "           'selected_frequency',\n",
    "           'tmin',\n",
    "           'tmax',\n",
    "           'detrend',\n",
    "           'reject',\n",
    "           'flat',\n",
    "           'ica_to_exclude',\n",
    "           'scaler', \n",
    "           'n_components',\n",
    "           'cov_est', \n",
    "           'log', \n",
    "           'model_type',\n",
    "           'trial_combo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e345e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:56.991331Z",
     "start_time": "2022-07-15T04:16:56.966393Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create dataframe with all combinations of tests as rows\n",
    "test_df = pd.DataFrame(itertools.product(l_freq_filter_options, \n",
    "                                         h_freq_filter_options, \n",
    "                                         channels_to_drop_options, \n",
    "                                         baseline_correction_options, \n",
    "                                         projectors_to_apply_options, \n",
    "                                         selected_frequency_options,\n",
    "                                         tmin_options,\n",
    "                                         tmax_options, \n",
    "                                         detrend_options,\n",
    "                                         reject_options,\n",
    "                                         flat_options,\n",
    "                                         ica_to_exclude_options,\n",
    "                                         scaler_options, \n",
    "                                         n_components_options, \n",
    "                                         cov_est_options, \n",
    "                                         log_options, \n",
    "                                         model_type_options, \n",
    "                                         trial_combo_options), \n",
    "                      columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a17e8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:57.007286Z",
     "start_time": "2022-07-15T04:16:56.999306Z"
    }
   },
   "outputs": [],
   "source": [
    "#Append columns for each subject, where we will record results for each test\n",
    "subject_columns = ['sub_A', 'sub_C', 'sub_D', 'sub_E', 'sub_F', 'sub_G', \n",
    "                   'sub_H', 'sub_J', 'sub_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95efa24b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:57.023241Z",
     "start_time": "2022-07-15T04:16:57.012269Z"
    }
   },
   "outputs": [],
   "source": [
    "#Add those combos to our test_df to save highest val accuracy achieved\n",
    "test_df = test_df.reindex(columns=columns + subject_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091cee81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T04:16:57.055156Z",
     "start_time": "2022-07-15T04:16:57.028228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1152, 27)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc48b6",
   "metadata": {},
   "source": [
    "### Let's test these options\n",
    "\n",
    "**set number of ICA components to create before running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fe4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(test_df.shape[0]):\n",
    "    #Load each sessions data into an MNE raw object\n",
    "    raw_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        raw_dict[key] = mne.io.RawArray(value.T, info, verbose=0)\n",
    "    \n",
    "    #Filter data with bandpass. Note raw.filter applies in place\n",
    "    for key, value in raw_dict.items():\n",
    "        value.filter(l_freq=test_df.l_freq_filter[row], \n",
    "                     h_freq=test_df.h_freq_filter[row], \n",
    "                     method='fir', phase='zero', verbose=0)\n",
    "\n",
    "    #Create epoch object with our raw objects and events arrays\n",
    "    channels_to_keep = [ch for ch in ch_names if \n",
    "                        ch not in test_df.channels_to_drop[row]]\n",
    "    epoch_dict = {}\n",
    "    for key, value in raw_dict.items():\n",
    "        epoch_dict[key] = mne.Epochs(value, events=event_dict[key], \n",
    "                                    event_id=events_explained, \n",
    "                                    tmin=-3, tmax=test_df.tmax[row], \n",
    "                                    baseline=test_df.baseline_correction[row],\n",
    "                                    preload=True,\n",
    "                                    picks=channels_to_keep, verbose=0,\n",
    "                                    detrend=test_df.detrend[row],\n",
    "                                    reject=test_df.reject[row],\n",
    "                                    flat=test_df.flat[row],\n",
    "                                    reject_tmin=test_df.tmin[row],\n",
    "                                    reject_tmax=test_df.tmax[row])\n",
    "\n",
    "    #Skip creating projectors step to save compute time if not being\n",
    "    #applied in this iteration\n",
    "    if test_df.projectors_to_apply[row]:\n",
    "        #Create dictionary of top 5 signal space projection vectors for each epoch\n",
    "        proj_dict = {}\n",
    "        for key, value in epoch_dict.items():\n",
    "            proj_dict[key] = mne.compute_proj_epochs(value, n_eeg=1, verbose=0)\n",
    "        #apply projectors\n",
    "        for key, value in epoch_dict.items():\n",
    "            value.add_proj(proj_dict[key][test_df.projectors_to_apply[row]], \n",
    "                           verbose=0)\n",
    "            value.apply_proj(verbose=0)\n",
    "\n",
    "    #Skip creating ICA components step to save compute time if not\n",
    "    #being applied in this iteration\n",
    "    if test_df.ica_to_exclude[row]:\n",
    "        #create and fit ICA object to epochs\n",
    "        for key, value in epoch_dict.items():\n",
    "            ica = mne.preprocessing.ICA(n_components=5, method='picard', \n",
    "                                        max_iter='auto', verbose=0)\n",
    "            ica.fit(value, verbose=0)\n",
    "            #Apply the ICA\n",
    "            ica.apply(value, exclude=test_df.ica_to_exclude[row],\n",
    "                     verbose=0)\n",
    "\n",
    "    #Resample the data at a new frequency\n",
    "    for key, value in epoch_dict.items():\n",
    "        value.resample(sfreq=test_df.selected_frequency[row])\n",
    "\n",
    "    #Extract and standard scale data from all non-dropped epochs\n",
    "    #Creates intermediate data dictionary\n",
    "    int_data_dict = {}\n",
    "    #Use robust sklearn scaler\n",
    "    if test_df.scaler[row] == 'robust':\n",
    "        mne_scaler = mne.decoding.Scaler(scalings='median')\n",
    "        for key, value in epoch_dict.items():\n",
    "            #with scalings=median implements sklearn robust scaler\n",
    "            int_data_dict[key] = (mne_scaler.\n",
    "                                  fit_transform(value.\n",
    "                                                get_data(tmin=test_df.tmin[row], \n",
    "                                                         tmax=test_df.tmax[row])))\n",
    "    #No scaling option\n",
    "    if test_df.scaler[row] is None:\n",
    "        for key, value in epoch_dict.items():\n",
    "            int_data_dict[key] = value.get_data(tmin=test_df.tmin[row], \n",
    "                                                  tmax=test_df.tmax[row])\n",
    "    \n",
    "    #Create updated dictionary of y values to reflect dropped epochs\n",
    "    int_y_dict = {}\n",
    "    for key, value in y_dict.items():\n",
    "        temp_y_list = []\n",
    "        for i, epoch in enumerate(epoch_dict[key].drop_log):\n",
    "    #MNE drop log shows empty parens for epochs that were not dropped - \n",
    "    #these are the trials we are keeping in each iteration\n",
    "            if epoch == ():\n",
    "                temp_y_list.append(value[i])\n",
    "        int_y_dict[key] = temp_y_list\n",
    "    \n",
    "    #Assemble final y dict with only trials in our current combo\n",
    "    #In each combo, coding 1st trial type to 0, 2nd trial type to 1\n",
    "    final_y_dict = {}\n",
    "    for key, value in int_y_dict.items():\n",
    "        temp_y_list = []\n",
    "        for y in value:\n",
    "            if y == test_df.trial_combo[row][0]:\n",
    "                temp_y_list.append(0)\n",
    "            if y == test_df.trial_combo[row][1]:\n",
    "                temp_y_list.append(1)\n",
    "        final_y_dict[key] = np.array(temp_y_list)\n",
    "\n",
    "    #Assemble data dict with only trials in our current combo\n",
    "    final_data_dict = {}\n",
    "    for key, value in int_data_dict.items():\n",
    "        index_list = []\n",
    "        for i, y in enumerate(int_y_dict[key]):\n",
    "            if (y == test_df.trial_combo[row][0] or \n",
    "                y == test_df.trial_combo[row][1]):\n",
    "                index_list.append(i)\n",
    "        final_data_dict[key] = value[index_list]\n",
    "\n",
    "    #Create csp_dict of csp objects\n",
    "    csp_dict = {}\n",
    "    for key, value in epoch_dict.items():\n",
    "        #Only want to create csp objects for our train data - from session 1\n",
    "        if 'sesh_1' in key:\n",
    "            csp_dict[key] = mne.decoding.CSP(n_components=int(test_df.n_components[row]), \n",
    "                                             cov_est=test_df.cov_est[row], \n",
    "                                             log=bool(test_df.log[row]));\n",
    "\n",
    "    #Suppress output from this noisy function with no verbose option\n",
    "    with io.capture_output() as captured:\n",
    "    #Fit csp objects to training data from session 1        \n",
    "        for key, value in csp_dict.items():\n",
    "            value.fit(X=final_data_dict[key], \n",
    "                      y=final_y_dict[key]);\n",
    "\n",
    "    #Use csp objects to transform and save resulting data\n",
    "    csp_data_dict = {}\n",
    "    for key, value in csp_dict.items():\n",
    "        csp_data_dict[key] = value.transform(final_data_dict[key]);\n",
    "        key2 = key.replace('1', '2')\n",
    "        csp_data_dict[key2] = value.transform(final_data_dict[key2]);\n",
    "\n",
    "\n",
    "    #Model against our data for each subject and save the resulting score\n",
    "    #First, LDA model\n",
    "    if test_df.model_type[row] == 'LDA':\n",
    "        #Base LDA objects on csp dict keys so only created for sesh 1\n",
    "        lda_dict = {}\n",
    "        for key in csp_dict.keys():\n",
    "            lda_dict[key] = LinearDiscriminantAnalysis()\n",
    "\n",
    "        #Fit LDA objects to training data from sesh 1\n",
    "        for key, value in lda_dict.items():\n",
    "            value.fit(csp_data_dict[key], final_y_dict[key])\n",
    "            \n",
    "        #Score on testing data from sesh 2 and save in test_df\n",
    "        for key, value in lda_dict.items():\n",
    "            key2 = key.replace('1', '2')\n",
    "            subject = key[:5]\n",
    "            test_df.at[row, subject] = value.score(csp_data_dict[key2], \n",
    "                                               final_y_dict[key2])\n",
    "    \n",
    "    #Neural network\n",
    "    if test_df.model_type[row] == 'NN':\n",
    "        #Build model for each subject\n",
    "        #Base NN analysis on csp dict keys so only created for sesh 1\n",
    "        for key in csp_dict.keys():      \n",
    "            #Build model\n",
    "            model = Sequential()\n",
    "            #inputs qre equal to n_components created via CSP\n",
    "            model.add(Dense(test_df.n_components[row], \n",
    "                               input_dim=test_df.n_components[row], \n",
    "                               activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "            #Add hidden layer with half as many nodes as input\n",
    "            model.add(Dense(test_df.n_components[row]/2, activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "            #Hidden layer with 1/4 as many nodes as input\n",
    "            model.add(Dense(test_df.n_components[row]/4, activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "            #output layer\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            #Compile model\n",
    "            model.compile(loss='binary_crossentropy', \n",
    "                          optimizer='adam', \n",
    "                          metrics=['acc'])\n",
    "            \n",
    "            #Fit model\n",
    "            key2 = key.replace('1', '2')\n",
    "            history = model.fit(csp_data_dict[key], final_y_dict[key], \n",
    "                                validation_data=(csp_data_dict[key2], \n",
    "                                                 final_y_dict[key2]), \n",
    "                                epochs=5, verbose=0)\n",
    "            \n",
    "            #Save validation accuracy into dataframe\n",
    "            subject = key[:5]\n",
    "            test_df.at[row, subject] = max(history.history['val_acc'])\n",
    "    test_df.to_csv('data/csp_grid_search.csv', index=False)\n",
    "    if row % 20 == 0:\n",
    "        print(f'Grid search complete through row {row} of {test_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef16fe",
   "metadata": {},
   "source": [
    "### Model refinement\n",
    "\n",
    "Ensemble model testing - later\n",
    "- Testing LDA on individual model vs NN on individual model vs NN on heigher sample weighted but all data ingested model\n",
    "\n",
    "**I think if transform into is CSP space it will yield data in a shape that I should still feed it into a CNN - don't think that really belongs in this test set. Do that separately.**\n",
    "\n",
    "**Grid search for reject and flat settings to maintain 90%, 75%, 60% of epochs for each trial participant, right now we're dropping way more with some participants**\n",
    "\n",
    "Grid search sending in a shorter period of data into the CSP\n",
    "\n",
    "Grid search parameters for LDA if it is performing better than simple NN\n",
    "\n",
    "Test pulling more time shifted samples\n",
    "\n",
    "5. Resampling our training data (e.g., including -0.3 to 4.9, 0. to 5.2, and 0.3 to 5.5) to give our model more data to train on\n",
    "\n",
    "When doing the individual models, I can set the rejection criteria for epochs to match the individual in question better - if I have time.\n",
    "\n",
    "Read this article for other preprocessing ideas after getting dropping epochs set up: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5915520/\n",
    "\n",
    "Another thing I could play around with:\n",
    "\n",
    "- EEGLib\n",
    "    - https://www.sciencedirect.com/science/article/pii/S2352711021000753\n",
    "    - Primarily used for feature extraction after the data has been processed, but it does have some preprocessing capability\n",
    "    - Appears to be written to allow visual inspection of data and then creation of features based on the selected point - certainly worth investigating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mne+tensorflow]",
   "language": "python",
   "name": "conda-env-mne_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
