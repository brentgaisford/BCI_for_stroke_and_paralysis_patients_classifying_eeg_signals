{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fa88b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T03:22:03.180849Z",
     "start_time": "2022-07-20T03:22:03.169880Z"
    }
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Flatten, Conv1D, \n",
    "                                     MaxPooling1D, GlobalAveragePooling1D)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "from IPython.utils import io\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab844a",
   "metadata": {},
   "source": [
    "### Purpose of this notebook\n",
    "\n",
    "In this notebook, we'll be testing and assembling the final ensemble model that we will use as our final predictor for each test subject.\n",
    "\n",
    "The ensemble model will include the following level 1 models for each subject:\n",
    "1. A CNN model with a larger filter size and fewer output nodes to capture larger features\n",
    "2. A CNN model with a smaller filter size and more output nodes to capture smaller features\n",
    "3. An LDA model with a small number of CSP components\n",
    "4. A NN looking at CSP transformed data\n",
    "\n",
    "First, as usual, we'll use our data ingester to bring our data into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d8c7204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T03:05:52.633175Z",
     "start_time": "2022-07-20T03:05:17.735539Z"
    }
   },
   "outputs": [],
   "source": [
    "%run data_ingester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea46e9e",
   "metadata": {},
   "source": [
    "### We will assemble an L1 model dataframe that generates all the inputs for our ensemble\n",
    "\n",
    "One row per model, so each subject will have multiple rows in the dataframe (one for each of the L1 model types). All L1 models for each subject will have the same general MNE preprocessing settings, so we can do the MNE preprocessing work just once for each subject - which would be extremely important in a production device to reduce computation time for ongoing predictions.\n",
    "\n",
    "In addition, we need to add a few columns to the dataframe:\n",
    "\n",
    "1. Model type ('LDA', 'NN_CSP' or 'CNN_LF', 'CNN_SF' respectively)\n",
    "2. train_output (array to be passed to ensemble model)\n",
    "    - For the LDA model this will be an array of probabilities between 0 and 1 with a length equal to the number of trials of the relevant trial type for that subject.\n",
    "    - For the CNN models and NN models this will be an array with a length equal to the number of trials, and a width equal to the number of nodes in the 2nd to last dense layer of the model\n",
    "3. test_output (array to be passed to ensemble model)\n",
    "    - Outputs are as above, but for either test data from session 1 (if this is not our final test on session 2 data), or output on session 2 data if the ensemble model is finalized and we are calculating our scores\n",
    "4. y_train and y_test\n",
    "    - We need to save the true values for the ensemble model to use in its fitting and scoring.\n",
    "\n",
    "\n",
    "### We will also have an ensemble_df\n",
    "\n",
    "This dataframe will contain combinations of our subjects and the permutations of the ensemble model parameters we want to test, as well as a columns where the ensemble model's scores can be recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494e836",
   "metadata": {},
   "source": [
    "**First let's import our csp models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ecaf172",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:46:51.215923Z",
     "start_time": "2022-07-20T07:46:51.186971Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import final CSP model params\n",
    "final_csp_models = pd.read_csv('data/csp_models_not_overfit.csv')\n",
    "\n",
    "#Reset several columns away from strings and as literal_evals so can be read\n",
    "final_csp_models['trial_combo'] = (final_csp_models['trial_combo'].\n",
    "                                apply(lambda x: literal_eval(x)))\n",
    "\n",
    "#Use list comprehension b/c NaN can appear in list\n",
    "final_csp_models['flat'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in final_csp_models['flat']]\n",
    "final_csp_models['reject'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in final_csp_models['reject']]\n",
    "\n",
    "#Add our three needed output columns\n",
    "final_csp_models['model_type'] = 'LDA'\n",
    "final_csp_models['train_output'] = None\n",
    "final_csp_models['test_output'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8f639",
   "metadata": {},
   "source": [
    "Unfortunately ast.literal_eval cannot evaluate slice expressions, which we also need to convert away from a string. I could write a complicated find expression to extract the data, but it is faster to just manually re-enter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "799d7e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:46:51.231852Z",
     "start_time": "2022-07-20T07:46:51.220882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    slice(None, 1, None)\n",
       "1    slice(None, 1, None)\n",
       "2    slice(None, 1, None)\n",
       "3       slice(1, 2, None)\n",
       "4    slice(None, 1, None)\n",
       "5    slice(None, 1, None)\n",
       "6    slice(None, 1, None)\n",
       "7    slice(None, 1, None)\n",
       "8       slice(1, 2, None)\n",
       "Name: projectors_to_apply, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_csp_models['projectors_to_apply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2ffe0429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:46:51.247813Z",
     "start_time": "2022-07-20T07:46:51.235846Z"
    }
   },
   "outputs": [],
   "source": [
    "final_csp_models['projectors_to_apply'] = [slice(None, 1, None),\n",
    "                                           slice(None, 1, None),\n",
    "                                           slice(None, 1, None),\n",
    "                                           slice(1, 2, None),\n",
    "                                           slice(1, 2, None),\n",
    "                                           slice(None, 1, None),\n",
    "                                           slice(None, 1, None),\n",
    "                                           slice(1, 2, None),\n",
    "                                           slice(1, 2, None)]\n",
    "\n",
    "final_csp_models['y_train'] = None\n",
    "final_csp_models['y_test'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84288bf4",
   "metadata": {},
   "source": [
    "**Let's import our CNN models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e7755bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:46:51.295684Z",
     "start_time": "2022-07-20T07:46:51.253791Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import final CNN model params\n",
    "cnn_models = pd.read_csv('data/CNN_models_most_general.csv')\n",
    "\n",
    "#Reset several columns away from strings and as literal_evals so can be read\n",
    "cnn_models['trial_combo'] = (cnn_models['trial_combo'].\n",
    "                                apply(lambda x: literal_eval(x)))\n",
    "\n",
    "#Use list comprehension b/c NaN can appear in list\n",
    "cnn_models['flat'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in cnn_models['flat']]\n",
    "cnn_models['reject'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in cnn_models['reject']]\n",
    "\n",
    "#Add our three needed output columns\n",
    "cnn_models['model_type'] = 'CNN'\n",
    "cnn_models['train_output'] = None\n",
    "cnn_models['test_output'] = None\n",
    "\n",
    "cnn_models['y_train'] = None\n",
    "cnn_models['y_test'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6dbd1",
   "metadata": {},
   "source": [
    "**And finally import our NN models run on CSP data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1abd7e0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:46:51.327433Z",
     "start_time": "2022-07-20T07:46:51.300491Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import final CNN model params\n",
    "nn_csp_models = pd.read_csv('data/NN_CSP_models_not_overfit.csv')\n",
    "\n",
    "#Reset several columns away from strings and as literal_evals so can be read\n",
    "nn_csp_models['trial_combo'] = (nn_csp_models['trial_combo'].\n",
    "                                apply(lambda x: literal_eval(x)))\n",
    "\n",
    "#Use list comprehension b/c NaN can appear in list\n",
    "nn_csp_models['flat'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in nn_csp_models['flat']]\n",
    "nn_csp_models['reject'] = [None if pd.isna(x) else literal_eval(x)\n",
    "                         for x in nn_csp_models['reject']]\n",
    "\n",
    "#Add our three needed output columns\n",
    "nn_csp_models['model_type'] = 'NN_CSP'\n",
    "nn_csp_models['train_output'] = None\n",
    "nn_csp_models['test_output'] = None\n",
    "\n",
    "nn_csp_models['y_train'] = None\n",
    "nn_csp_models['y_test'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6baf49",
   "metadata": {},
   "source": [
    "**Combine into L1 model dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b1a57233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:46:51.359354Z",
     "start_time": "2022-07-20T07:46:51.331411Z"
    }
   },
   "outputs": [],
   "source": [
    "L1_model_df = pd.concat((final_csp_models, \n",
    "                         cnn_models, \n",
    "                         nn_csp_models), \n",
    "                        ignore_index=True)\n",
    "\n",
    "#Replace NaN with None so MNE can read it\n",
    "L1_model_df.replace(np.nan, None, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ff39a",
   "metadata": {},
   "source": [
    "### Let's construct the ensemble_df test frame\n",
    "Each row of this frame will use a different set of parameters for the final ensemble model construction, and have space for the resulting training and test scores to be recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "055ab048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T06:51:00.742401Z",
     "start_time": "2022-07-20T06:51:00.703487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 10)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layers_options = [1, 2]\n",
    "l2_reg_alpha_options = [0.0001, 0.001]\n",
    "dropout_ratio_options = [0.2, 0.4]\n",
    "#How number of nodes reduces with each dense layer\n",
    "dense_reduction_ratio_options = [0, 0.25]\n",
    "#How many epochs to run through\n",
    "epochs_options = [3, 6]\n",
    "\n",
    "\n",
    "columns = ['hidden_layers',\n",
    "           'l2_reg_alpha',\n",
    "           'dropout_ratio',\n",
    "           'dense_reduction',\n",
    "           'epochs']\n",
    "\n",
    "#Create dataframe of our first set of variable params\n",
    "ensemble_test_df = pd.DataFrame(itertools.\n",
    "                               product(hidden_layers_options,\n",
    "                                       l2_reg_alpha_options,\n",
    "                                       dropout_ratio_options,\n",
    "                                       dense_reduction_ratio_options,\n",
    "                                       epochs_options), \n",
    "                           columns=columns)\n",
    "\n",
    "#Create dataframe with just our subjects, add results columns\n",
    "temp_dict = {'subject': final_csp_models.subject.unique()}\n",
    "temp_frame = pd.DataFrame(temp_dict)\n",
    "temp_frame['train_score'] = None\n",
    "temp_frame['test_score'] = None\n",
    "temp_frame['true_pos_rate'] = None\n",
    "temp_frame['true_neg_rate'] = None\n",
    "\n",
    "#Get number of permutations of variable parameters\n",
    "permutations = len(list(itertools.\n",
    "                        product(hidden_layers_options,\n",
    "                                l2_reg_alpha_options,\n",
    "                                dropout_ratio_options,\n",
    "                                dense_reduction_ratio_options,\n",
    "                                epochs_options)))\n",
    "\n",
    "#Duplicate our temp frame to match the number of variable\n",
    "#permutations to run each permutation for each subject\n",
    "temp_columns = temp_frame.columns\n",
    "temp_frame = pd.DataFrame(np.repeat(temp_frame.values, \n",
    "                                     permutations, \n",
    "                                     axis=0))\n",
    "temp_frame.columns = temp_columns\n",
    "\n",
    "#Concat variable params with itself 9 times times to get\n",
    "#right shape to combine with all params for all subjects\n",
    "ensemble_test_df = pd.concat((ensemble_test_df, \n",
    "                              ensemble_test_df,\n",
    "                              ensemble_test_df,\n",
    "                              ensemble_test_df,\n",
    "                              ensemble_test_df,\n",
    "                              ensemble_test_df,\n",
    "                              ensemble_test_df,\n",
    "                              ensemble_test_df,\n",
    "                              ensemble_test_df),\n",
    "                              ignore_index=True)\n",
    "\n",
    "#Join our two grids to assemble full ensemble test grid\n",
    "ensemble_test_df = temp_frame.join(ensemble_test_df)\n",
    "\n",
    "ensemble_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1d5fa",
   "metadata": {},
   "source": [
    "### Build the functions that will build and test our ensemble models\n",
    "\n",
    "The has two nested iterators:\n",
    "\n",
    "First, it iterates through the subjects. All of the L1 models for each subject use the same MNE parameters, so we can do those operations first and use the results for all rows where that subject is mentioned.\n",
    "\n",
    "Then, it iterates through the rows which match that subject and performs the needed calculations to create the correct output for the L1 model on that row.\n",
    "\n",
    "Finally, when it has iterated through all the rows for that individual, it perfoms the ensemble calculations at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0382e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T05:20:52.799379Z",
     "start_time": "2022-07-20T05:20:52.703637Z"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble_input_creator(L1_model_df, final_test, savefile):\n",
    "    \"\"\"A function to create all the needed inputs for our ensemble\n",
    "    model inside the L1_model_df.\n",
    "    \n",
    "    The L1_model_df should have one row per L1 model, and specify the\n",
    "    subject to which that L1 model applies. All L1 models for each subject\n",
    "    must use the same parameters for MNE preprocessing steps.\n",
    "    \n",
    "    final_test: True, False. When set to False, a train-test split\n",
    "    will be conducted on session 1 data and the saved score in the \n",
    "    ensemble_df accuracy column for each subject will be the \n",
    "    score on the test data from session 1. When set to True, the model\n",
    "    will be trained on the entirety of the session 1 data, and the test score\n",
    "    shown is the final accuracy against unseen session 2 data.\"\"\"\n",
    "    \n",
    "    \n",
    "    #First we populate our L1_model_df with output from each L1 model \n",
    "    #to be used as input into each ensemble model\n",
    "    \n",
    "    #For each row in ensemble we iterate across all subjects\n",
    "    for sub in L1_model_df['subject'].unique():\n",
    "\n",
    "        #Create list of index numbers for all L1 models for this subject\n",
    "        i_list = L1_model_df.loc[L1_model_df['subject'] == sub].index\n",
    "\n",
    "        #First we will perform the MNE preprocessing for this subject\n",
    "        #All L1 models for each subject use same MNE params, so we'll\n",
    "        #refer to the first row for this subject\n",
    "        i1 = i_list[0]\n",
    "\n",
    "        #Load each sessions data into an MNE raw object\n",
    "        raw_dict = {}\n",
    "        for key, value in data_dict.items():\n",
    "            raw_dict[key] = mne.io.RawArray(value.T, info, verbose=0)\n",
    "\n",
    "        #Filter data with bandpass. Note raw.filter applies in place\n",
    "        for key, value in raw_dict.items():\n",
    "            value.filter(l_freq=L1_model_df.l_freq_filter[i1], \n",
    "                         h_freq=L1_model_df.h_freq_filter[i1], \n",
    "                         method='fir', phase='zero', verbose=0)\n",
    "\n",
    "        #Create epoch object with our raw objects and events arrays\n",
    "        channels_to_keep = [ch for ch in ch_names if \n",
    "                            ch not in L1_model_df.channels_to_drop[i1]]\n",
    "        epoch_dict = {}\n",
    "        for key, value in raw_dict.items():\n",
    "            epoch_dict[key] = mne.Epochs(value, events=event_dict[key], \n",
    "                                        event_id=events_explained, \n",
    "                                        tmin=-3, \n",
    "                                        tmax=L1_model_df.tmax[i1], \n",
    "                                        baseline=L1_model_df.baseline_correction[i1],\n",
    "                                        preload=True,\n",
    "                                        picks=channels_to_keep, verbose=0,\n",
    "                                        detrend=L1_model_df.detrend[i1],\n",
    "                                        reject=L1_model_df.reject[i1],\n",
    "                                        flat=L1_model_df.flat[i1],\n",
    "                                        reject_tmin=L1_model_df.tmin[i1],\n",
    "                                        reject_tmax=L1_model_df.tmax[i1])\n",
    "\n",
    "        #Skip creating projectors step to save compute time if not being\n",
    "        #applied in this iteration\n",
    "        if L1_model_df.projectors_to_apply[i1]:\n",
    "            #Create dictionary of signal space projection vectors for each epoch\n",
    "            proj_dict = {}\n",
    "            for key, value in epoch_dict.items():\n",
    "                proj_dict[key] = mne.compute_proj_epochs(value, \n",
    "                                                         n_eeg=2, \n",
    "                                                         verbose=0)\n",
    "            #apply projectors\n",
    "            for key, value in epoch_dict.items():\n",
    "                value.add_proj(proj_dict[key][L1_model_df.projectors_to_apply[i1]], \n",
    "                               verbose=0)\n",
    "                value.apply_proj(verbose=0)\n",
    "\n",
    "        #Skip creating ICA components step to save compute time if not\n",
    "        #being applied in this iteration\n",
    "        if L1_model_df.ica_to_exclude[i1]:\n",
    "            #create and fit ICA object to epochs\n",
    "            for key, value in epoch_dict.items():\n",
    "                ica = mne.preprocessing.ICA(n_components=5, method='picard', \n",
    "                                            max_iter='auto', verbose=0)\n",
    "                ica.fit(value, verbose=0)\n",
    "                #Apply the ICA\n",
    "                ica.apply(value, exclude=L1_model_df.ica_to_exclude[i1],\n",
    "                         verbose=0)\n",
    "\n",
    "        #Resample the data at a new frequency, happens inplace\n",
    "        for key, value in epoch_dict.items():\n",
    "            value.resample(sfreq=L1_model_df.selected_frequency[i1])\n",
    "\n",
    "        #Extract and standard scale data from all non-dropped epochs\n",
    "        #Creates intermediate data dictionary\n",
    "        int_data_dict = {}\n",
    "        #Use robust sklearn scaler\n",
    "        if L1_model_df.scaler[i1] == 'robust':\n",
    "            mne_scaler = mne.decoding.Scaler(scalings='median')\n",
    "            for key, value in epoch_dict.items():\n",
    "                #with scalings=median implements sklearn robust scaler\n",
    "                int_data_dict[key] = (mne_scaler.\n",
    "                                      fit_transform(value.\n",
    "                                                    get_data(tmin=L1_model_df.tmin[i1], \n",
    "                                                             tmax=L1_model_df.tmax[i1])))\n",
    "        #No scaling option\n",
    "        if L1_model_df.scaler[i1] is None:\n",
    "            for key, value in epoch_dict.items():\n",
    "                int_data_dict[key] = value.get_data(tmin=L1_model_df.tmin[i1], \n",
    "                                                      tmax=L1_model_df.tmax[i1])\n",
    "\n",
    "        #Create updated dictionary of y values to reflect dropped epochs\n",
    "        int_y_dict = {}\n",
    "        for key, value in y_dict.items():\n",
    "            temp_y_list = []\n",
    "            for i, epoch in enumerate(epoch_dict[key].drop_log):\n",
    "                #MNE drop log shows empty parens for epochs that were not dropped - \n",
    "                #these are the trials we are keeping in each iteration\n",
    "                if epoch == ():\n",
    "                    temp_y_list.append(value[i])\n",
    "            int_y_dict[key] = temp_y_list\n",
    "\n",
    "        #Assemble final y dict with only trials in our current combo\n",
    "        #In each combo, coding 1st trial type to 0, 2nd trial type to 1\n",
    "        final_y_dict = {}\n",
    "        for key, value in int_y_dict.items():\n",
    "            temp_y_list = []\n",
    "            for y in value:\n",
    "                if y == L1_model_df.trial_combo[i1][0]:\n",
    "                    temp_y_list.append(0)\n",
    "                if y == L1_model_df.trial_combo[i1][1]:\n",
    "                    temp_y_list.append(1)\n",
    "            final_y_dict[key] = np.array(temp_y_list)\n",
    "\n",
    "        #Assemble data dict with only trials in our current combo\n",
    "        final_data_dict = {}\n",
    "        for key, value in int_data_dict.items():\n",
    "            index_list = []\n",
    "            for i, y in enumerate(int_y_dict[key]):\n",
    "                if (y == L1_model_df.trial_combo[i1][0] or \n",
    "                    y == L1_model_df.trial_combo[i1][1]):\n",
    "                    index_list.append(i)\n",
    "            final_data_dict[key] = value[index_list]\n",
    "\n",
    "        #If this isn't our final test on sesh 2 data,\n",
    "        #We need to train test split our sesh 1 data\n",
    "        if final_test == False:\n",
    "            #Train test split the sesh 1 data and y for the subject of current row\n",
    "            for key, value in final_data_dict.items():\n",
    "                if (sub in key) and ('sesh_1' in key):\n",
    "                    sub_X = value\n",
    "            for key, value in final_y_dict.items():\n",
    "                if (sub in key) and ('sesh_1' in key):\n",
    "                    sub_y = value\n",
    "            (X_train, X_test, \n",
    "             y_train, y_test) = train_test_split(sub_X, \n",
    "                                                 sub_y, \n",
    "                                                 stratify=sub_y,\n",
    "                                                 random_state=23)\n",
    "\n",
    "            #Save y_train and y_test into df for ensemble model to use\n",
    "            L1_model_df.at[i1, 'y_train'] = y_train\n",
    "            L1_model_df.at[i1, 'y_test'] = y_test\n",
    "            \n",
    "\n",
    "            #For subject, reset X and y in final dicts with train values,\n",
    "            #test values will be passed to L1 models for validation\n",
    "            for key, value in final_data_dict.items():\n",
    "                if (sub in key) and ('sesh_1' in key):\n",
    "                    final_data_dict[key] = X_train\n",
    "            for key, value in final_y_dict.items():\n",
    "                if (sub in key) and ('sesh_1' in key):\n",
    "                    final_y_dict[key] = y_train\n",
    "                    \n",
    "        if final_test == True:\n",
    "            #Need to save down true y's for the ensemble model to fit and score\n",
    "            for key, value in final_y_dict.items():\n",
    "                if (sub in key) and ('sesh_1' in key):\n",
    "                    L1_model_df.at[i1, 'y_train'] = value\n",
    "                if (sub in key) and ('sesh_2' in key):\n",
    "                    L1_model_df.at[i1, 'y_test'] = value\n",
    "                    \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        #We're now complete with the pre-processing common to all models\n",
    "        #And will begin iterating over specific rows for this subject\n",
    "        for row in i_list:\n",
    "        \n",
    "        \n",
    "        \n",
    "            #If this is a model built on top of CSP, do CSP preprocessing\n",
    "            if ((L1_model_df.model_type[row] == 'LDA') or \n",
    "                (L1_model_df.model_type[row] == 'NN_CSP')):\n",
    "                #Create csp_dict of csp objects\n",
    "                csp_dict = {}\n",
    "                for key, value in epoch_dict.items():\n",
    "                    #Only need to create CSP objects for our subject,\n",
    "                    #and only in session 1\n",
    "                    if (sub in key) and ('sesh_1' in key):\n",
    "                        csp_dict[key] = mne.decoding.CSP(n_components=int(L1_model_df.n_components[row]), \n",
    "                                                         cov_est=L1_model_df.cov_est[row], \n",
    "                                                         log=bool(L1_model_df.log[row]));\n",
    "\n",
    "                #Suppress output from this noisy function\n",
    "                with io.capture_output() as captured:\n",
    "                #Fit csp objects to training data from session 1        \n",
    "                    for key, value in csp_dict.items():\n",
    "                        value.fit(X=final_data_dict[key], \n",
    "                                  y=final_y_dict[key]);\n",
    "\n",
    "                #Use csp object to transform data\n",
    "                csp_data_dict = {}\n",
    "                for key, value in csp_dict.items():\n",
    "                    #Use CSP object from sesh_1 to tranform both seshs data\n",
    "                    csp_data_dict[key] = value.transform(final_data_dict[key])\n",
    "                    key2 = key.replace('sesh_1', 'sesh_2')\n",
    "                    csp_data_dict[key2] = value.transform(final_data_dict[key2])\n",
    "                    #In non-final tests also need to transform X_test\n",
    "                    #CSP tranform happens in place so need to deepcopy X_test\n",
    "                    if final_test == False:\n",
    "                        X_test_CSP = copy.deepcopy(X_test)\n",
    "                        X_test_CSP = value.transform(X_test_CSP)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "            #Its now time to iterate through each of the L1 models for this subject\n",
    "            #Recall i_list is the indexes of all rows in L1 models for this subject\n",
    "            #First, the LDA models        \n",
    "            if L1_model_df.model_type[row] == 'LDA':\n",
    "                #Create LDA object for our subject (just one, fed from csp_dict)\n",
    "                for key, value in csp_dict.items():\n",
    "                    LDA = LinearDiscriminantAnalysis()\n",
    "                    #Remember train data assigned to dicts when final_test=False\n",
    "                    LDA.fit(csp_data_dict[key], final_y_dict[key])\n",
    "\n",
    "                    if final_test == False:\n",
    "                        #Save down probabilities for train and test\n",
    "                        L1_model_df.at[row, 'train_output'] = LDA.predict_proba(csp_data_dict[key])\n",
    "                        L1_model_df.at[row, 'test_output'] = LDA.predict_proba(X_test_CSP)\n",
    "\n",
    "                    #For final tests, fit and score against sesh1, sesh2\n",
    "                    if final_test == True:\n",
    "                        L1_model_df.at[row, 'train_output'] = LDA.predict_proba(csp_data_dict[key])\n",
    "                        key2 = key.replace('sesh_1', 'sesh_2')\n",
    "                        L1_model_df.at[row, 'test_output'] = LDA.predict_proba(csp_data_dict[key2])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #Now for our NN models on CSP data:\n",
    "            if L1_model_df.model_type[row] == 'NN_CSP':\n",
    "                #Model against our data for each subject and save the resulting score\n",
    "                for key, value in csp_dict.items():\n",
    "\n",
    "                    #Build model\n",
    "                    model = Sequential()\n",
    "                    #inputs are equal to n_components created via CSP\n",
    "                    model.add(Dense(int(L1_model_df.n_components[row]), \n",
    "                                    input_dim=int(L1_model_df.n_components[row]), \n",
    "                                    activation='relu'))\n",
    "                    model.add(Dropout(0.2))\n",
    "                    #Add hidden layer with half as many nodes as input\n",
    "                    model.add(Dense(int(L1_model_df.n_components[row]/2), \n",
    "                                    activation='relu'))\n",
    "                    model.add(Dropout(0.2))\n",
    "                    #Hidden layer with 1/4 as many nodes as input\n",
    "                    model.add(Dense(int(L1_model_df.n_components[row]/4), \n",
    "                                    activation='relu'))\n",
    "                    model.add(Dropout(0.2))\n",
    "                    #output layer\n",
    "                    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "                    #Suppress output\n",
    "                    with io.capture_output() as captured:\n",
    "                        #Compile model\n",
    "                        model.compile(loss='binary_crossentropy', \n",
    "                                      optimizer='adam', \n",
    "                                      metrics=['acc'])\n",
    "\n",
    "                        #Fit model\n",
    "                        history = model.fit(csp_data_dict[key], \n",
    "                                            final_y_dict[key], \n",
    "                                            epochs=3, verbose=0)\n",
    "\n",
    "                    #Create extractor to save output of penultimate dense layer\n",
    "                    extractor = Model(inputs=model.inputs,\n",
    "                                      outputs=model.get_layer(index=-3).output)\n",
    "                    \n",
    "                    if final_test == False:\n",
    "                    #Save down probabilities for train and test\n",
    "                        L1_model_df.at[row, \n",
    "                                       'train_output'] = np.array(extractor(csp_data_dict[key]))\n",
    "                        L1_model_df.at[row, \n",
    "                                       'test_output'] = np.array(extractor(X_test_CSP))\n",
    "                        \n",
    "\n",
    "                    #For final tests, fit and score against sesh1, sesh2\n",
    "                    if final_test == True:\n",
    "                        L1_model_df.at[row, \n",
    "                                       'train_output'] = np.array(extractor(csp_data_dict[key]))\n",
    "                        key2 = key.replace('1', '2')\n",
    "                        L1_model_df.at[row, \n",
    "                                       'test_output'] = np.array(extractor(csp_data_dict[key2]))\n",
    "                                                                  \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #Now for our CNN models\n",
    "            if L1_model_df.model_type[row] == 'CNN':\n",
    "\n",
    "                #Create sample weight lists, higher weight for subject\n",
    "                #Remember final_y_dict has correct y_train whether final or not\n",
    "                sample_weights = []\n",
    "                for key, value in final_y_dict.items():\n",
    "                    if (sub in key) and ('sesh_1' in key):\n",
    "                        temp = [L1_model_df.sample_weight[row]] * len(value)\n",
    "                        sample_weights += temp\n",
    "                    if (sub not in key) and ('sesh_1' in key):\n",
    "                        temp = [1] * len(value)\n",
    "                        sample_weights += temp\n",
    "                        \n",
    "                if final_test == True:\n",
    "                    #Need to get X_test and y_test into variables\n",
    "                    for key, value in final_y_dict.items():\n",
    "                        if (sub in key) and ('sesh_2' in key):\n",
    "                            y_test = value\n",
    "                    for key, value in final_data_dict.items():\n",
    "                        if (sub in key) and ('sesh_2' in key):\n",
    "                            X_test = value\n",
    "                \n",
    "                #Concat all sesh 1 values together into X_train and y_train for CNNs\n",
    "                #Remember for both final_test instances these are correct\n",
    "                X_train = np.concatenate(([final_data_dict[key] for \n",
    "                                           key in final_data_dict.keys()\n",
    "                                          if 'sesh_1' in key]), axis=0)\n",
    "                y_train = np.concatenate(([final_y_dict[key] for \n",
    "                                           key in final_y_dict.keys()\n",
    "                                          if 'sesh_1' in key]))\n",
    "\n",
    "\n",
    "                #Reshape all data tensors to feed into neural network\n",
    "                #Rename result to avoid overwriting X_test and y_test\n",
    "                #Otherwise misaligns shapes when final_test=False and\n",
    "                #More than 1 CNN model per subject\n",
    "                X_train_NN = np.reshape(X_train, (X_train.shape[0],\n",
    "                                               X_train.shape[2],\n",
    "                                               X_train.shape[1]))\n",
    "                X_test_NN = np.reshape(X_test, (X_test.shape[0],\n",
    "                                             X_test.shape[2],\n",
    "                                             X_test.shape[1]))\n",
    "                y_train_NN = np.reshape(y_train, len(y_train))\n",
    "                y_test_NN = np.reshape(y_test, len(y_test))\n",
    "                sample_weights = np.reshape(sample_weights, \n",
    "                                            len(sample_weights))\n",
    "\n",
    "\n",
    "                #Model against our data and save the resulting val score\n",
    "                model = Sequential()\n",
    "                model.add(Conv1D(filters=L1_model_df.input_filter_count[row], \n",
    "                                 kernel_size=int(L1_model_df.input_kernel_size[row]), \n",
    "                                 strides=int(L1_model_df.input_strides[row]), \n",
    "                                 padding='same', \n",
    "                                 activation='relu', \n",
    "                                 input_shape=(X_train_NN.shape[1], \n",
    "                                              X_train_NN.shape[2])))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(MaxPooling1D(int(L1_model_df.pool_size[row])))\n",
    "                model.add(Conv1D(filters=L1_model_df.hidden_filters[row], \n",
    "                                 kernel_size=int(L1_model_df.hidden_kernel_size[row]), \n",
    "                                 strides=int(L1_model_df.hidden_strides[row]), \n",
    "                                 padding='same', \n",
    "                                 activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(MaxPooling1D(int(L1_model_df.pool_size[row])))\n",
    "                model.add(GlobalAveragePooling1D())\n",
    "                model.add(Dense(L1_model_df.nodes[row], activation='relu'))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "                #compile & fit model\n",
    "                model.compile(loss='binary_crossentropy', \n",
    "                              optimizer='adam', \n",
    "                              weighted_metrics=['accuracy'])\n",
    "                #Suppress output\n",
    "                with io.capture_output() as captured:\n",
    "                    history = model.fit(x=X_train_NN, \n",
    "                                        y=y_train_NN, \n",
    "                                        sample_weight=sample_weights, \n",
    "                                        batch_size=60, epochs=3, \n",
    "                                        validation_data=(X_test_NN, \n",
    "                                                         y_test_NN), \n",
    "                                        verbose=0, workers=8)\n",
    "\n",
    "                #Save down the output of our penultimate dense layer\n",
    "                extractor = Model(inputs=model.inputs,\n",
    "                                  outputs=model.get_layer(index=-3).output)\n",
    "                L1_model_df.at[row, 'test_output'] = (np.array\n",
    "                                                      (extractor(X_test_NN)))\n",
    "\n",
    "                #Run model on just data for subject, not all sesh 1\n",
    "                #To get train output for ensemble\n",
    "                for key, value in final_data_dict.items():\n",
    "                        if (sub in key) and ('sesh_1' in key):\n",
    "                            X_train_small = value\n",
    "                #Reshape X_train_small to feed into neural network\n",
    "                X_train_small = np.reshape(X_train_small, \n",
    "                                           (X_train_small.shape[0],\n",
    "                                            X_train_small.shape[2],\n",
    "                                            X_train_small.shape[1]))\n",
    "                L1_model_df.at[row, 'train_output'] = (np.array\n",
    "                                                       (extractor\n",
    "                                                        (X_train_small)))\n",
    "                \n",
    "                \n",
    "    #Save the results down\n",
    "    L1_model_df.to_csv(f'data/{savefile}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aecbe4",
   "metadata": {},
   "source": [
    "### Let's create our ensemble training and final scoring function\n",
    "\n",
    "This function will enable us to train our ensemble model, and then calculate our final scores against either test data, or, on our final test, data from session 2 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f5d1a49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T06:50:53.482311Z",
     "start_time": "2022-07-20T06:50:53.442401Z"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble_test(L1_model_df, ensemble_df, savefile):\n",
    "    \"\"\"Uses the outputs of the L1 model to train ensemble model.\n",
    "    \n",
    "    The ensemble_df dataframe should include the settings we want to\n",
    "    test for the ensemble model, and has columns to fill in the resulting\n",
    "    accuracy on the test set.\n",
    "    \n",
    "    This function iterates through all rows in the ensemble_df.\n",
    "    On each row it applies the ensemble parameters listed on \n",
    "    that row to construct a NN model, and then uses that model\n",
    "    to make a final prediction for each subject. The model is\n",
    "    fit using the L1 outputs in the L1_model_df, and a score is\n",
    "    then recorded for each subject.\"\"\"\n",
    "    for row in range(ensemble_df.shape[0]):\n",
    "        #Get subject of current row\n",
    "        sub = ensemble_df['subject'][row]\n",
    "        \n",
    "        #Pull all needed inputs from L1_model_df\n",
    "        #y values come from first row of L1 matching subject \n",
    "        y_train = (L1_model_df.loc[L1_model_df['subject'] == sub, \n",
    "                                   'y_train'].values[0])\n",
    "        y_test = (L1_model_df.loc[L1_model_df['subject'] == sub, \n",
    "                                   'y_test'].values[0])\n",
    "        #Reshape y test and train to feed into NN\n",
    "        y_train = np.reshape(y_train, len(y_train))\n",
    "        y_test = np.reshape(y_test, len(y_test))\n",
    "        \n",
    "        #Concatenate X_train and test from all matching sub rows of L1 DF\n",
    "        X_train = L1_model_df.loc[L1_model_df['subject'] == sub, \n",
    "                                  'train_output'].values\n",
    "        X_test = L1_model_df.loc[L1_model_df['subject'] == sub, \n",
    "                                  'test_output'].values\n",
    "        \n",
    "        #Concatenate together into single array for entry into NN\n",
    "        X_train = np.concatenate((X_train), axis=1)\n",
    "        X_test = np.concatenate((X_test), axis=1)\n",
    "        \n",
    "        \n",
    "        #Now let's build our NN to make final predictions\n",
    "        model = Sequential()\n",
    "        \n",
    "        #Input nodes in first layer is equal to combined input nodes from L1\n",
    "        input_nodes = X_train.shape[1]\n",
    "        \n",
    "        #Add first two layers\n",
    "        model.add(Dense(input_nodes, \n",
    "                        input_dim=input_nodes, \n",
    "                        activation='relu', \n",
    "                        kernel_regularizer=l2(ensemble_df.l2_reg_alpha[row])))\n",
    "        model.add(Dropout(ensemble_df.dropout_ratio[row]))\n",
    "        model.add(Dense(int((1 - ensemble_df.dense_reduction[row]) * \n",
    "                             input_nodes), \n",
    "                        activation='relu', \n",
    "                        kernel_regularizer=l2(ensemble_df.l2_reg_alpha[row])))\n",
    "\n",
    "        #Add 1-2 more dense hidden layers depending on settings\n",
    "        if ensemble_df.hidden_layers[row] >= 2:\n",
    "            model.add(Dropout(ensemble_df.dropout_ratio[row]))\n",
    "            model.add(Dense(int(((1 - ensemble_df.dense_reduction[row])**2) * \n",
    "                             input_nodes), \n",
    "                            activation='relu', \n",
    "                            kernel_regularizer=l2(ensemble_df.l2_reg_alpha[row])))\n",
    "        if ensemble_df.hidden_layers[row] >= 3:\n",
    "            model.add(Dropout(ensemble_df.dropout_ratio[row]))\n",
    "            model.add(Dense(int(((1 - ensemble_df.dense_reduction[row])**3) * \n",
    "                             input_nodes), \n",
    "                            activation='relu', \n",
    "                            kernel_regularizer=l2(ensemble_df.l2_reg_alpha[row])))\n",
    "        \n",
    "        #Add output layer\n",
    "        model.add(Dropout(ensemble_df.dropout_ratio[row]))\n",
    "        model.add(Dense(1, \n",
    "                        activation='sigmoid', \n",
    "                        kernel_regularizer=l2(ensemble_df.l2_reg_alpha[row])))\n",
    "\n",
    "        # Compile it\n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['acc'])\n",
    "\n",
    "        # Fit it\n",
    "        history = model.fit(X_train, y_train, \n",
    "                            validation_data=(X_test, y_test), \n",
    "                            epochs=ensemble_df.epochs[row], \n",
    "                            verbose=0)\n",
    "        \n",
    "        #Save the best val and training accuracy achieved by our model\n",
    "        ensemble_df.at[row, 'train_score'] = max(history.history['acc'])\n",
    "        ensemble_df.at[row, 'test_score'] = max(history.history['val_acc'])\n",
    "        \n",
    "        #Calculate true positive and negative rates\n",
    "        #First generate predictions\n",
    "        test_pred = model.predict(X_test, verbose=0).flatten()\n",
    "        \n",
    "        #Round those prediction and convert to integers\n",
    "        test_pred = [int(round(x)) for x in test_pred]\n",
    "\n",
    "        #Then calculate rates of true positivity and negativity\n",
    "        ensemble_df.at[row, 'true_pos_rate'] = (sum([1 for pred, actual in \n",
    "                                                    zip(test_pred, y_test)\n",
    "                                                    if (pred == actual) &\n",
    "                                                     (actual == 1)])\n",
    "                                                / sum(y_test))\n",
    "        ensemble_df.at[row, 'true_neg_rate'] = (sum([1 for pred, actual in \n",
    "                                                    zip(test_pred, y_test)\n",
    "                                                    if (pred == actual) &\n",
    "                                                     (actual == 0)])\n",
    "                                                / (len(y_test) - \n",
    "                                                   sum(y_test)))\n",
    "        \n",
    "        ensemble_df.to_csv(f'data/{savefile}.csv', index=False)\n",
    "        if row % 50 == 0:\n",
    "            print(f'Grid search complete through row {row} of {ensemble_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cfb3a",
   "metadata": {},
   "source": [
    "### Let's run our functions and look at our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0006ebfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T06:41:28.275047Z",
     "start_time": "2022-07-20T06:18:52.685953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete through row 0 of 288\n",
      "Grid search complete through row 50 of 288\n",
      "Grid search complete through row 100 of 288\n",
      "Grid search complete through row 150 of 288\n",
      "Grid search complete through row 200 of 288\n",
      "Grid search complete through row 250 of 288\n"
     ]
    }
   ],
   "source": [
    "ensemble_input_creator(L1_model_df=L1_model_df, \n",
    "                       final_test=False,\n",
    "                       savefile='L1_inputs_to_ensemble')\n",
    "ensemble_test(L1_model_df=L1_model_df, \n",
    "              ensemble_df=ensemble_test_df,\n",
    "              savefile='ensemble_grid_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "63ac0dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:03:24.139675Z",
     "start_time": "2022-07-20T07:03:24.069861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>true_pos_rate</th>\n",
       "      <th>true_neg_rate</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>l2_reg_alpha</th>\n",
       "      <th>dropout_ratio</th>\n",
       "      <th>dense_reduction</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>sub_J</td>\n",
       "      <td>0.87037</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>sub_J</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>sub_J</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>sub_E</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>sub_F</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>sub_J</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subject train_score test_score true_pos_rate true_neg_rate  hidden_layers  \\\n",
       "73    sub_D        0.95        1.0           1.0           1.0              1   \n",
       "81    sub_D    0.916667        1.0           1.0           1.0              2   \n",
       "91    sub_D         0.9        1.0           1.0           1.0              2   \n",
       "67    sub_D    0.883333        0.9           0.8           1.0              1   \n",
       "249   sub_J     0.87037   0.736842      0.777778           0.7              2   \n",
       "89    sub_D    0.866667        0.9           0.8           1.0              2   \n",
       "75    sub_D    0.866667        1.0           1.0           1.0              1   \n",
       "77    sub_D        0.85        0.9           0.8           1.0              1   \n",
       "235   sub_J    0.833333   0.631579      0.555556           0.7              1   \n",
       "93    sub_D    0.816667       0.95           0.9           0.9              2   \n",
       "85    sub_D    0.816667        0.9           0.8           1.0              2   \n",
       "233   sub_J    0.814815   0.684211      0.666667           0.6              1   \n",
       "99    sub_E    0.811321   0.722222      0.555556      0.888889              1   \n",
       "83    sub_D         0.8       0.95           1.0           0.9              2   \n",
       "65    sub_D         0.8        0.7           0.4           1.0              1   \n",
       "72    sub_D         0.8        0.9           0.9           0.9              1   \n",
       "69    sub_D         0.8        1.0           1.0           1.0              1   \n",
       "136   sub_F    0.796296   0.666667         0.375           0.9              1   \n",
       "241   sub_J    0.796296   0.736842      0.888889           0.5              2   \n",
       "257   sub_L    0.793103        0.9           0.9           0.9              1   \n",
       "\n",
       "     l2_reg_alpha  dropout_ratio  dense_reduction  epochs  \n",
       "73         0.0010            0.2             0.00       6  \n",
       "81         0.0001            0.2             0.00       6  \n",
       "91         0.0010            0.2             0.25       6  \n",
       "67         0.0001            0.2             0.25       6  \n",
       "249        0.0010            0.2             0.00       6  \n",
       "89         0.0010            0.2             0.00       6  \n",
       "75         0.0010            0.2             0.25       6  \n",
       "77         0.0010            0.4             0.00       6  \n",
       "235        0.0010            0.2             0.25       6  \n",
       "93         0.0010            0.4             0.00       6  \n",
       "85         0.0001            0.4             0.00       6  \n",
       "233        0.0010            0.2             0.00       6  \n",
       "99         0.0001            0.2             0.25       6  \n",
       "83         0.0001            0.2             0.25       6  \n",
       "65         0.0001            0.2             0.00       6  \n",
       "72         0.0010            0.2             0.00       3  \n",
       "69         0.0001            0.4             0.00       6  \n",
       "136        0.0010            0.2             0.00       3  \n",
       "241        0.0001            0.2             0.00       6  \n",
       "257        0.0001            0.2             0.00       6  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_test_df.sort_values('train_score', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49154fc",
   "metadata": {},
   "source": [
    "**OK, let's modify our ensemble settings to what seem to be the best options for not overfitting, and run our final model against the session 2 data**\n",
    "\n",
    "Unfortunately we didn't get very clear signal from running our ensemble against test data. Many of our models have fairly wide divergence between train and test scores, and subject D in particular appears set to be wildly overfit on session 1 data. Its performing very well on our test split, but I'd be very surprised if it did as well against our real session 2 data.\n",
    "\n",
    "I suspect I'm going to need to drop the CNN models - based on everything I've seen to this point, they are causing a large part of the overfitting on session 1 data.\n",
    "\n",
    "I'm locking in settings for my ensemble model and going to run my models against the real test data from session 2 - but am anticipating re-running afterward with just my CSP based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad51737b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:04:59.350724Z",
     "start_time": "2022-07-20T07:04:59.308839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 10)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layers_options = [1]\n",
    "l2_reg_alpha_options = [0.001]\n",
    "dropout_ratio_options = [0.2]\n",
    "#How number of nodes reduces with each dense layer\n",
    "dense_reduction_ratio_options = [0]\n",
    "#How many epochs to run through\n",
    "epochs_options = [3]\n",
    "\n",
    "\n",
    "columns = ['hidden_layers',\n",
    "           'l2_reg_alpha',\n",
    "           'dropout_ratio',\n",
    "           'dense_reduction',\n",
    "           'epochs']\n",
    "\n",
    "#Create dataframe of our first set of variable params\n",
    "ensemble_test_df_2 = pd.DataFrame(itertools.\n",
    "                               product(hidden_layers_options,\n",
    "                                       l2_reg_alpha_options,\n",
    "                                       dropout_ratio_options,\n",
    "                                       dense_reduction_ratio_options,\n",
    "                                       epochs_options), \n",
    "                           columns=columns)\n",
    "\n",
    "#Create dataframe with just our subjects, add results columns\n",
    "temp_dict = {'subject': final_csp_models.subject.unique()}\n",
    "temp_frame = pd.DataFrame(temp_dict)\n",
    "temp_frame['train_score'] = None\n",
    "temp_frame['test_score'] = None\n",
    "temp_frame['true_pos_rate'] = None\n",
    "temp_frame['true_neg_rate'] = None\n",
    "\n",
    "#Get number of permutations of variable parameters\n",
    "permutations = len(list(itertools.\n",
    "                        product(hidden_layers_options,\n",
    "                                l2_reg_alpha_options,\n",
    "                                dropout_ratio_options,\n",
    "                                dense_reduction_ratio_options,\n",
    "                                epochs_options)))\n",
    "\n",
    "#Duplicate our temp frame to match the number of variable\n",
    "#permutations to run each permutation for each subject\n",
    "temp_columns = temp_frame.columns\n",
    "temp_frame = pd.DataFrame(np.repeat(temp_frame.values, \n",
    "                                     permutations, \n",
    "                                     axis=0))\n",
    "temp_frame.columns = temp_columns\n",
    "\n",
    "#Concat variable params with itself 9 times times to get\n",
    "#right shape to combine with all params for all subjects\n",
    "ensemble_test_df_2 = pd.concat((ensemble_test_df_2, \n",
    "                              ensemble_test_df_2,\n",
    "                              ensemble_test_df_2,\n",
    "                              ensemble_test_df_2,\n",
    "                              ensemble_test_df_2,\n",
    "                              ensemble_test_df_2,\n",
    "                              ensemble_test_df_2,\n",
    "                              ensemble_test_df_2,\n",
    "                              ensemble_test_df_2),\n",
    "                              ignore_index=True)\n",
    "\n",
    "#Join our two grids to assemble full ensemble test grid\n",
    "ensemble_test_df_2 = temp_frame.join(ensemble_test_df_2)\n",
    "\n",
    "ensemble_test_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "49c37eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:15:53.817180Z",
     "start_time": "2022-07-20T07:05:17.957839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search complete through row 0 of 9\n"
     ]
    }
   ],
   "source": [
    "ensemble_input_creator(L1_model_df=L1_model_df, \n",
    "                       final_test=True,\n",
    "                       savefile='L1_inputs_to_ensemble_final_run_all_4')\n",
    "ensemble_test(L1_model_df=L1_model_df, \n",
    "              ensemble_df=ensemble_test_df_2,\n",
    "              savefile='ensemble_grid_search_final_run_all_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9262251a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:16:07.077984Z",
     "start_time": "2022-07-20T07:16:07.046072Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>true_pos_rate</th>\n",
       "      <th>true_neg_rate</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>l2_reg_alpha</th>\n",
       "      <th>dropout_ratio</th>\n",
       "      <th>dense_reduction</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub_J</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub_C</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub_F</td>\n",
       "      <td>0.712329</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.72973</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sub_G</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub_L</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub_D</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sub_H</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub_E</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub_A</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject train_score test_score true_pos_rate true_neg_rate  hidden_layers  \\\n",
       "7   sub_J    0.739726       0.72      0.972222      0.487179              1   \n",
       "1   sub_C      0.5375   0.658228         0.475      0.846154              1   \n",
       "4   sub_F    0.712329   0.585714       0.72973      0.272727              1   \n",
       "5   sub_G    0.526316   0.529412           1.0           0.0              1   \n",
       "8   sub_L    0.701299   0.514286      0.540541      0.363636              1   \n",
       "2   sub_D      0.7375        0.5           0.0           1.0              1   \n",
       "6   sub_H        0.55        0.5      0.885714      0.057143              1   \n",
       "3   sub_E    0.557143   0.492308           0.0           1.0              1   \n",
       "0   sub_A    0.619718   0.487179      0.871795      0.102564              1   \n",
       "\n",
       "   l2_reg_alpha  dropout_ratio  dense_reduction  epochs  \n",
       "7         0.001            0.2                0       3  \n",
       "1         0.001            0.2                0       3  \n",
       "4         0.001            0.2                0       3  \n",
       "5         0.001            0.2                0       3  \n",
       "8         0.001            0.2                0       3  \n",
       "2         0.001            0.2                0       3  \n",
       "6         0.001            0.2                0       3  \n",
       "3         0.001            0.2                0       3  \n",
       "0         0.001            0.2                0       3  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_test_df_2.sort_values('test_score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd94e933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T07:28:01.621547Z",
     "start_time": "2022-07-20T07:28:01.596590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5541252096494039"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_test_df_2.test_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f1c5d",
   "metadata": {},
   "source": [
    "**As I suspected, it looks like we still overfit to session 1 badly**\n",
    "\n",
    "Subject D is the best evidence of this - that model was achieving 100% accuracy with more epochs on the training data, but is no better than a coinflip on session 2 data - it predicted 0 every time on the test data after the shifts to the EEG signal in session 2. Unfortunately our models just are not solving this problem well - a general CNN or LDA applied to all subjects would likely perform better. More work is needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mne+tensorflow]",
   "language": "python",
   "name": "conda-env-mne_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
